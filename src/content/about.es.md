# Sobre LATAM Leaderboard

LATAM Leaderboard es una iniciativa comunitaria dedicada a establecer estándares de evaluación transparentes y orientados a tareas para sistemas de IA en América Latina. Estamos ayudando a construir un motor que mide trabajo real, no solo fluidez lingüística.

LATAM Leaderboard es más que un sistema de rankings, es la base para posicionar a nuestra región como un hub global de innovación en IA. Seas investigador, desarrollador o entusiasta de la IA, hay un lugar para vos en la construcción de estos estándares de evaluación.

## Nuestra Misión

Creamos benchmarks rigurosos, específicos para la región y orientados a tareas, en español y portugués y a través de industrias de LATAM, para que investigadores, desarrolladores y empresas puedan entrenar, elegir y desplegar modelos que realmente marquen la diferencia en producción.

## Por qué Importa

### De la Cultura a la Capacidad

La evaluación en IA ha estado dominada por benchmarks centrados en el inglés que no capturan los requisitos de desempeño matizados de los mercados latinoamericanos.

Estamos cambiando eso al crear estándares rigurosos y específicos para la región que empoderan a nuestra comunidad a construir y elegir soluciones de IA con confianza.

### Señales para el Aprendizaje (y RL)

Capturar las idiosincrasias regionales es esencial, pero no suficiente. El principal cuello de botella es la evaluación en aplicaciones reales: sin señales precisas a nivel de tarea, los modelos no pueden aprender de forma confiable, seguir restricciones o entregar resultados productivos. A medida que el entrenamiento se desplaza hacia métodos de refuerzo y basados en retroalimentación, necesitamos criterios de éxito claros y auditables. Nuestros benchmarks están diseñados para que sus resultados sirvan como señales de entrenamiento/validación, haciendo explícito qué representa "hacerlo bien" en tareas de LATAM.

### Cerrando la Brecha de Evaluación

Las evaluaciones de alta calidad en español y portugués son escasas, especialmente para tareas concretas. Esa brecha bloquea decisiones informadas y ralentiza el progreso de los modelos en casos de uso reales de LATAM.

## Cómo Evaluamos Modelos

Ejecutamos benchmarks de acceso público basados en múltiples forks de lm-evaluation-harness, con foco en:

- Definiciones de tareas con criterios de aceptación (qué cuenta como éxito).
- Entornos reproducibles (configs versionadas, semillas y datos).
- Evaluación orientada a resultados (éxito en la tarea más métricas de apoyo apropiadas para cada caso).
- Reportes transparentes (qué cubre el benchmark, y qué no).

### Estándares en Evolución

- Fase 1: Benchmarks integrales de comprensión del lenguaje.
- Fase 2: Evaluación de tareas del mundo real (ej. traducción, transcripción, resumen, seguimiento de instrucciones y salidas estructuradas).
- Fase 3: Benchmarks y datasets especializados aportados por la comunidad, con formatos de retroalimentación adecuados para entrenamiento y flujos de RL.

## Colaboración Comunitaria

### Para Product Managers de IA (AI PMs)

Traducir resultados de negocio en benchmarks y señales de entrenamiento. Definir qué significa "bueno" para guiar los modelos hacia KPIs reales.

### Para Investigadores y Universidades

Contribuir con benchmarks, metodologías y datasets. Ayudar a establecer estándares académicos alineados con necesidades reales de la región.

### Para Desarrolladores de Modelos

Mostrar el desempeño de los modelos en tareas específicas de la región. Obtener información accionable sobre fortalezas, debilidades y dónde enfocar más entrenamiento.

### Para Empresas y Profesionales

Acceder a datos de desempeño confiables para guiar la implementación. Proponer tus tareas reales: te ayudamos a expresarlas como benchmarks claros con criterios de aceptación.

## Nuestro Compromiso

- **Ciencia Abierta:** Benchmarks, código y resultados son públicos por defecto.
- **Rigor Académico:** Metodología transparente y alcance cuidadoso—sin "teatro de leaderboards".
- **Enfoque Regional:** Construido alrededor de los idiomas y tareas de LATAM que impulsan la productividad.
- **Propiedad Comunitaria:** Mantenemos la infraestructura; la comunidad define los estándares.

## Ayudá a Construir el Futuro de la IA en LATAM

LATAM Leaderboard es más que un ranking, es la capa de evaluación que permite a nuestra región entrenar y desplegar sistemas que resuelvan problemas reales.

## ¿Listo para contribuir?

LATAM Leaderboard cuenta con el apoyo de infraestructura de cómputo de Surus, junto con contribuciones de desarrollo y metodología de la comunidad de IA de LATAM.