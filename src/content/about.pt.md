# Sobre o LATAM Leaderboard

LATAM Leaderboard é uma iniciativa comunitária dedicada a estabelecer padrões de avaliação transparentes e orientados a tarefas para sistemas de IA que servem a América Latina. Estamos ajudando a construir o motor para medir trabalho real, não apenas fluência linguística.

LATAM Leaderboard é mais do que um sistema de classificação, é a base para estabelecer nossa região como um hub global de inovação em IA. Seja você pesquisador, desenvolvedor ou entusiasta de IA, há um lugar para você na construção desses padrões de avaliação.

## Nossa Missão

Criamos benchmarks rigorosos, específicos da região e orientados a tarefas, em espanhol e português e em indústrias da América Latina, para que pesquisadores, desenvolvedores e empresas possam treinar, escolher e implementar modelos que realmente façam a diferença em produção.

## Por que Isso Importa

### Da Cultura à Capacidade

A avaliação de IA tem sido dominada por benchmarks centrados no inglês que falham em capturar os requisitos de desempenho nuançados dos mercados latino-americanos. Estamos mudando isso criando padrões rigorosos e específicos da região que capacitam nossa comunidade a construir e escolher soluções de IA com confiança.

Capturar idiossincrasias regionais é essencial, mas não suficiente. O principal gargalo é a avaliação: sem sinais precisos no nível de tarefas, modelos não podem aprender instruções de forma confiável, seguir restrições ou entregar resultados de produção.

### Sinais para Aprendizagem (e RL)

À medida que o treinamento muda para métodos de reforço e baseados em feedback, precisamos de critérios de sucesso claros e auditáveis. Nossos benchmarks são projetados para que seus resultados possam servir como sinais de treinamento/validação—tornando explícito o que "bom" significa para tarefas da América Latina.

### Fechando a Lacuna de Avaliação

Avaliações de alta qualidade em espanhol e português são escassas, especialmente para tarefas concretas. Essa lacuna bloqueia decisões informadas e retarda o progresso do modelo para casos de uso reais da América Latina.

## Como Avaliamos Modelos

Executamos benchmarks publicamente acessíveis baseados em múltiplos forks do lm-evaluation-harness, focando em:

- Definições de tarefas com critérios de aceitação (o que conta como sucesso).
- Configurações reproduzíveis (configs versionadas, seeds e dados).
- Pontuação orientada a resultados (sucesso da tarefa mais métricas de suporte apropriadas para cada tarefa).
- Relatórios transparentes (o que o benchmark cobre—e o que não cobre).

### Padrões em Evolução

- Fase 1: Benchmarks abrangentes de compreensão de linguagem.
- Fase 2: Avaliação de tarefas do mundo real (ex. tradução, transcrição, resumo, seguimento de instruções e saídas estruturadas).
- Fase 3: Benchmarks e datasets especializados contribuídos pela comunidade, com formatos de feedback adequados para fluxos de treinamento e RL.

## Colaboração Comunitária

### Para Gerentes de Produto de IA (AI PMs)

Traduzir resultados de negócio em benchmarks e sinais de treinamento. Definir o que "bom" significa para que modelos possam ser direcionados para KPIs reais.

### Para Pesquisadores e Universidades

Contribuir com benchmarks, metodologias e datasets. Ajudar a estabelecer padrões acadêmicos que se alinhem com necessidades regionais reais.

### Para Desenvolvedores de Modelos

Mostrar desempenho do modelo em tarefas específicas da região. Obter insights acionáveis sobre pontos fortes, fracos e onde focar treinamento adicional.

### Para Empresas e Profissionais

Acessar dados de desempenho confiáveis para guiar implementação. Proponha suas tarefas reais—ajudaremos a expressá-las como benchmarks claros com critérios de aceitação.

## Nosso Compromisso

- **Ciência Aberta:** Benchmarks, código e resultados são públicos por padrão.
- **Rigor Acadêmico:** Metodologia transparente e escopo cuidadoso—sem teatro de leaderboard.
- **Foco Regional:** Construído em torno das linguagens e tarefas da América Latina que impulsionam a produtividade.
- **Propriedade Comunitária:** Mantemos a infraestrutura; a comunidade molda os padrões.

## Ajude a Construir o Futuro da IA da América Latina

LATAM Leaderboard é mais do que um ranking, é a camada de avaliação que permite à nossa região treinar e implementar sistemas que resolvem problemas reais.

## Pronto para contribuir?

LATAM Leaderboard é orgulhosamente apoiado por infraestrutura de computação da Surus, com contribuições de desenvolvimento e metodologia da comunidade de IA da América Latina.