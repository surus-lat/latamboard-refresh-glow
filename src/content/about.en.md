# About LatamBoard

LatamBoard is a community initiative dedicated to establishing task-oriented, transparent evaluation standards for AI systems serving Latin America. We're helping build the engine to measure real work, not just language fluency.

LatamBoard is more than a ranking system, it's the foundation for establishing our region as a global AI innovation hub. Whether you're a researcher, developer, or AI enthusiast, there's a place for you in building these evaluation standards.

## Our Mission

We create rigorous, region-specific, task-oriented benchmarks, in Spanish and Portuguese and across LATAM industries, so researchers, developers, and companies can train, choose, and ship models that actually move the needle in production.

## Why This Matters

### From Culture to Capability

AI evaluation has been dominated by English-centric benchmarks that fail to capture the nuanced performance requirements of Latin American markets. We're changing that by creating rigorous, region-specific evaluation standards that empower our community to build and choose AI solutions with confidence.

Capturing regional idiosyncrasies is essential but not sufficient. The main bottleneck is evaluation: without precise, task-level signals, models can't reliably learn instructions, follow constraints, or deliver production outcomes.

### Signals for Learning (and RL)

As training shifts toward reinforcement and feedback-driven methods, we need clear, auditable success criteria. Our benchmarks are designed so their results can serve as training/validation signals—making "what good looks like" explicit for LATAM tasks.

### Bridging the Evaluation Gap

High-quality evaluations in Spanish and Portuguese are scarce, especially for concrete tasks. That gap blocks informed decisions and slows model progress for real LATAM use cases.

## How We Evaluate Models

We run publicly accessible benchmarks based on multiple forks of lm-evaluation-harness, focusing on:

- Task definitions with acceptance criteria (what counts as success).
- Reproducible setups (versioned configs, seeds, and data).
- Outcome-first scoring (task success plus supporting metrics appropriate to each task).
- Transparent reports (what the benchmark covers—and what it doesn't).

### Evolving Standards

- Phase 1: Comprehensive language understanding benchmarks.
- Phase 2: Real-world task evaluation (e.g., translation, transcription, summarization, instruction following, and structured outputs).
- Phase 3: Community-contributed benchmarks and specialized datasets, with feedback formats suitable for training and RL workflows.

## Community Collaboration

### For AI Product Managers (AI PMs)

Translate business outcomes into benchmarks and training signals. Define what "good" means so models can be steered toward real KPIs.

### For Researchers & Universities

Contribute benchmarks, methodologies, and datasets. Help set academic standards that align with real regional needs.

### For Model Developers

Showcase model performance on region-specific tasks. Get actionable insight into strengths, weaknesses, and where to focus further training.

### For Companies & Practitioners

Access reliable performance data to guide implementation. Propose your actual tasks—we'll help express them as clear benchmarks with acceptance criteria.

## Our Commitment

- **Open Science:** Benchmarks, code, and results are public by default.
- **Academic Rigor:** Transparent methodology and careful scope—no leaderboard theater.
- **Regional Focus:** Built around LATAM languages and the tasks that drive productivity.
- **Community Ownership:** We maintain the infrastructure; the community shapes the standards.

## Help Build Latin America's AI Future

LatamBoard is more than a ranking, it's the evaluation layer that lets our region train and deploy systems that solve real problems.

## Ready to contribute?

LatamBoard is proudly supported by compute infrastructure from Surus, with development and methodology contributions from the broader LATAM AI community.