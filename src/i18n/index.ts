export type Locale = 'en' | 'es' | 'pt'

export const DEFAULT_LOCALE: Locale = 'en'

export type TranslationDict = Record<string, string | TranslationDict>

export const en: TranslationDict = {
  common: {
    app_name: 'LATAM Leaderboard',
    home: 'Home',
    tasks: 'Tasks',
    about: 'About',
    submit: 'Submit',
    our_website: 'Our Website',
    join_discord: 'Join our Discord',
    contact_email: 'contacto@surus.dev',
    loading: 'Loading…',
    error_generic: 'Something went wrong. Try again.',
  },
  landing: {
    hero_title: 'LATAM Leaderboard',
    hero_subtitle:
      "The community-driven platform for evaluating AI models on Spanish and Portuguese benchmarks. Advancing AI excellence across Latin America through transparent, rigorous evaluation.",
    source_prefix: 'Source:',
    source_link: 'Hugging Face dataset',
    load_failed: 'Failed to load leaderboard data',
    columns: {
      model_name: 'model_name',
      overall_latam_score: 'overall_latam_score',
      spanish_score: 'spanish_score',
      portuguese_score: 'portuguese_score',
    },
  },
  tests: {
    title: 'Tasks',
    loading: 'Loading tasks…',
    repo: 'Repository',
    key: 'Key',
    shots: 'shots',
    failed_meta: 'Failed to load tasks metadata',
    dataset: 'Dataset',
  },
  submit: {
    suggest_model: 'Suggest a Model',
    model_name: 'Model name',
    precision: 'Precision',
    email: 'Email',
    model_placeholder: 'org/model',
    precision_placeholder: 'e.g. bf16, fp8',
    email_placeholder: 'you@example.com',
    submit: 'Submit',
    submitting: 'Submitting…',
    thanks_touch: "Thanks! We'll be in touch.",
    thanks_review: "Thanks! We'll review it.",
    suggest_task: 'Suggest a Task/Dataset',
    task_key: 'Task key',
    task_name: 'Task name',
    group: 'Category / Group',
    select: 'Select…',
    spanish_group: 'Spanish (latam_es)',
    portuguese_group: 'Portuguese (latam_pr)',
    other_group: 'Other (latam_ts)',
    dataset_url: 'Dataset URL',
    url_placeholder: 'https://huggingface.co/datasets/...',
    short_desc: 'Short description',
    short_desc_placeholder: 'What does this task evaluate?',
    contact_email_label: 'Contact email',
    submit_task: 'Submit task',
    not_sure: 'Not sure how to start?',
    get_in_touch: 'Get in touch',
  },
  about: {
    title: 'About LATAM Leaderboard',
    p1:
      "LATAM Leaderboard is a community initiative dedicated to establishing task-oriented, transparent evaluation standards for AI systems serving Latin America. We're helping build the engine to measure real work, not just language fluency.",
    p2:
      "LATAM Leaderboard is more than a ranking system, it's the foundation for establishing our region as a global AI innovation hub. Whether you're a researcher, developer, or AI enthusiast, there's a place for you in building these evaluation standards.",
    mission_title: 'Our Mission',
    mission_p:
      'We create rigorous, region-specific, task-oriented benchmarks, in Spanish and Portuguese and across LATAM industries, so researchers, developers, and companies can train, choose, and ship models that actually move the needle in production.',
    why_title: 'Why This Matters',
    culture_sub: 'From Culture to Capability',
    culture_p1:
      "AI evaluation has been dominated by English-centric benchmarks that fail to capture the nuanced performance requirements of Latin American markets. We're changing that by creating rigorous, region-specific evaluation standards that empower our community to build and choose AI solutions with confidence.",
    culture_p2:
      "Capturing regional idiosyncrasies is essential but not sufficient. The main bottleneck is evaluation: without precise, task-level signals, models can't reliably learn instructions, follow constraints, or deliver production outcomes.",
    signals_sub: 'Signals for Learning (and RL)',
    signals_p:
      'As training shifts toward reinforcement and feedback-driven methods, we need clear, auditable success criteria. Our benchmarks are designed so their results can serve as training/validation signals—making "what good looks like" explicit for LATAM tasks.',
    bridge_sub: 'Bridging the Evaluation Gap',
    bridge_p:
      "High-quality evaluations in Spanish and Portuguese are scarce, especially for concrete tasks. That gap blocks informed decisions and slows model progress for real LATAM use cases.",
    how_title: 'How We Evaluate Models',
    how_p: 'We run publicly accessible benchmarks based on multiple forks of lm-evaluation-harness, focusing on:',
    how_li1: 'Task definitions with acceptance criteria (what counts as success).',
    how_li2: 'Reproducible setups (versioned configs, seeds, and data).',
    how_li3: 'Outcome-first scoring (task success plus supporting metrics appropriate to each task).',
    how_li4: "Transparent reports (what the benchmark covers—and what it doesn't).",
    evolving_title: 'Evolving Standards',
    evolving_li1: 'Phase 1: Comprehensive language understanding benchmarks.',
    evolving_li2:
      'Phase 2: Real-world task evaluation (e.g., translation, transcription, summarization, instruction following, and structured outputs).',
    evolving_li3:
      'Phase 3: Community-contributed benchmarks and specialized datasets, with feedback formats suitable for training and RL workflows.',
    community_title: 'Community Collaboration',
    aips_sub: 'For AI Product Managers (AI PMs)',
    aips_p: 'Translate business outcomes into benchmarks and training signals. Define what "good" means so models can be steered toward real KPIs.',
    research_sub: 'For Researchers & Universities',
    research_p: 'Contribute benchmarks, methodologies, and datasets. Help set academic standards that align with real regional needs.',
    devs_sub: 'For Model Developers',
    devs_p:
      'Showcase model performance on region-specific tasks. Get actionable insight into strengths, weaknesses, and where to focus further training.',
    companies_sub: 'For Companies & Practitioners',
    companies_p:
      'Access reliable performance data to guide implementation. Propose your actual tasks—we\'ll help express them as clear benchmarks with acceptance criteria.',
    commit_title: 'Our Commitment',
    commit_li1: 'Open Science: Benchmarks, code, and results are public by default.',
    commit_li2: 'Academic Rigor: Transparent methodology and careful scope—no leaderboard theater.',
    commit_li3: 'Regional Focus: Built around LATAM languages and the tasks that drive productivity.',
    commit_li4: 'Community Ownership: We maintain the infrastructure; the community shapes the standards.',
    future_title: "Help Build Latin America's AI Future",
    future_p:
      "LATAM Leaderboard is more than a ranking, it's the evaluation layer that lets our region train and deploy systems that solve real problems.",
    ready_title: 'Ready to contribute?',
    ready_p:
      'LATAM Leaderboard is proudly supported by compute infrastructure from Surus, with development and methodology contributions from the broader LATAM AI community.',
  },
}

export const es: TranslationDict = {
  common: {
    app_name: 'LATAM Leaderboard',
    home: 'Inicio',
    tasks: 'Tareas',
    about: 'Acerca de',
    submit: 'Enviar',
    our_website: 'Nuestro sitio web',
    join_discord: 'Únete a nuestro Discord',
    contact_email: 'contacto@surus.dev',
    loading: 'Cargando…',
    error_generic: 'Algo salió mal. Intenta de nuevo.',
  },
  landing: {
    hero_title: 'LATAM Leaderboard',
    hero_subtitle:
      'Plataforma comunitaria para evaluar modelos de IA en español y portugués. Impulsando la excelencia en IA en América Latina con evaluación rigurosa y transparente.',
    source_prefix: 'Fuente:',
    source_link: 'Dataset en Hugging Face',
    load_failed: 'Error al cargar los datos del leaderboard',
    columns: {
      model_name: 'model_name',
      overall_latam_score: 'overall_latam_score',
      spanish_score: 'spanish_score',
      portuguese_score: 'portuguese_score',
    },
  },
  tests: {
    title: 'Tareas',
    loading: 'Cargando tareas…',
    repo: 'Repositorio',
    key: 'Clave',
    shots: 'disparos',
    failed_meta: 'Error al cargar los metadatos de tareas',
    dataset: 'Dataset',
  },
  submit: {
    suggest_model: 'Sugerir un modelo',
    model_name: 'Nombre del modelo',
    precision: 'Precisión',
    email: 'Correo electrónico',
    model_placeholder: 'org/model',
    precision_placeholder: 'p. ej., bf16, fp8',
    email_placeholder: 'tu@ejemplo.com',
    submit: 'Enviar',
    submitting: 'Enviando…',
    thanks_touch: '¡Gracias! Nos pondremos en contacto.',
    thanks_review: '¡Gracias! Lo revisaremos.',
    suggest_task: 'Sugerir una tarea/dataset',
    task_key: 'Clave de la tarea',
    task_name: 'Nombre de la tarea',
    group: 'Categoría / Grupo',
    select: 'Seleccionar…',
    spanish_group: 'Español (latam_es)',
    portuguese_group: 'Portugués (latam_pr)',
    other_group: 'Otro (latam_ts)',
    dataset_url: 'URL del dataset',
    url_placeholder: 'https://huggingface.co/datasets/...',
    short_desc: 'Descripción breve',
    short_desc_placeholder: '¿Qué evalúa esta tarea?',
    contact_email_label: 'Correo de contacto',
    submit_task: 'Enviar tarea',
    not_sure: '¿No sabes por dónde empezar?',
    get_in_touch: 'Contáctanos',
  },
  about: {
    title: 'Acerca de LATAM Leaderboard',
    p1:
      'LATAM Leaderboard es una iniciativa comunitaria dedicada a establecer estándares de evaluación orientados a tareas y transparentes para sistemas de IA en América Latina. Ayudamos a medir trabajo real, no solo fluidez lingüística.',
    p2:
      'LATAM Leaderboard es más que un ranking; es la base para posicionar a nuestra región como un polo mundial de innovación en IA. Ya seas investigador, desarrollador o entusiasta, hay un lugar para ti en la construcción de estos estándares.',
    mission_title: 'Nuestra misión',
    mission_p:
      'Creamos benchmarks rigurosos, específicos de la región y orientados a tareas, en español y portugués y a través de industrias LATAM, para que investigadores, desarrolladores y empresas puedan entrenar, elegir y desplegar modelos que de verdad generen impacto.',
    why_title: 'Por qué importa',
    culture_sub: 'De la cultura a la capacidad',
    culture_p1:
      'La evaluación de IA ha estado dominada por benchmarks centrados en inglés que no capturan los requisitos de rendimiento de los mercados latinoamericanos. Cambiamos eso con estándares rigurosos y específicos de la región.',
    culture_p2:
      'Capturar idiosincrasias regionales es esencial pero insuficiente. El cuello de botella es la evaluación: sin señales precisas a nivel de tarea, los modelos no aprenden a seguir instrucciones ni a cumplir restricciones.',
    signals_sub: 'Señales para el aprendizaje (y RL)',
    signals_p:
      'A medida que el entrenamiento se orienta al refuerzo y la retroalimentación, necesitamos criterios de éxito claros y auditables. Nuestros benchmarks permiten usar sus resultados como señales de entrenamiento/validación.',
    bridge_sub: 'Reduciendo la brecha de evaluación',
    bridge_p:
      'Las evaluaciones de calidad en español y portugués son escasas, especialmente para tareas concretas. Esa brecha bloquea decisiones informadas y ralentiza el progreso.',
    how_title: 'Cómo evaluamos modelos',
    how_p: 'Ejecutamos benchmarks públicos basados en forks de lm-evaluation-harness, enfocándonos en:',
    how_li1: 'Definiciones de tareas con criterios de aceptación.',
    how_li2: 'Configuraciones reproducibles (versionado, seeds y datos).',
    how_li3: 'Puntajes orientados a resultados.',
    how_li4: 'Reportes transparentes (qué cubre y qué no).',
    evolving_title: 'Estándares en evolución',
    evolving_li1: 'Fase 1: Benchmarks de comprensión lingüística.',
    evolving_li2: 'Fase 2: Evaluación de tareas reales (traducción, transcripción, resumen, instrucciones y salidas estructuradas).',
    evolving_li3: 'Fase 3: Benchmarks de la comunidad y datasets especializados, con formatos útiles para entrenamiento y RL.',
    community_title: 'Colaboración comunitaria',
    aips_sub: 'Para PMs de IA',
    aips_p: 'Traduce resultados de negocio a benchmarks y señales de entrenamiento.',
    research_sub: 'Para investigadores y universidades',
    research_p: 'Contribuye benchmarks, metodologías y datasets alineados con necesidades reales.',
    devs_sub: 'Para desarrolladores de modelos',
    devs_p: 'Muestra desempeño en tareas regionales y obtén insights accionables.',
    companies_sub: 'Para empresas y practicantes',
    companies_p: 'Accede a datos confiables para guiar implementación. Propón tus tareas reales.',
    commit_title: 'Nuestro compromiso',
    commit_li1: 'Ciencia abierta: benchmarks, código y resultados públicos por defecto.',
    commit_li2: 'Rigor académico: metodología transparente; nada de teatro de leaderboard.',
    commit_li3: 'Enfoque regional: centrado en lenguas y tareas productivas.',
    commit_li4: 'Propiedad comunitaria: la comunidad define los estándares.',
    future_title: 'Construyamos el futuro de la IA en LATAM',
    future_p: 'Más que un ranking, somos la capa de evaluación para desplegar soluciones reales.',
    ready_title: '¿Listo para contribuir?',
    ready_p: 'Con apoyo de Surus y la comunidad LATAM de IA.',
  },
}

export const pt: TranslationDict = {
  common: {
    app_name: 'LATAM Leaderboard',
    home: 'Início',
    tasks: 'Tarefas',
    about: 'Sobre',
    submit: 'Enviar',
    our_website: 'Nosso site',
    join_discord: 'Entre no nosso Discord',
    contact_email: 'contacto@surus.dev',
    loading: 'Carregando…',
    error_generic: 'Algo deu errado. Tente novamente.',
  },
  landing: {
    hero_title: 'LATAM Leaderboard',
    hero_subtitle:
      'Plataforma comunitária para avaliar modelos de IA em espanhol e português. Promovendo excelência em IA na América Latina com avaliação rigorosa e transparente.',
    source_prefix: 'Fonte:',
    source_link: 'Dataset no Hugging Face',
    load_failed: 'Falha ao carregar os dados do leaderboard',
    columns: {
      model_name: 'model_name',
      overall_latam_score: 'overall_latam_score',
      spanish_score: 'spanish_score',
      portuguese_score: 'portuguese_score',
    },
  },
  tests: {
    title: 'Tarefas',
    loading: 'Carregando tarefas…',
    repo: 'Repositório',
    key: 'Chave',
    shots: 'exemplos',
    failed_meta: 'Falha ao carregar metadados das tarefas',
    dataset: 'Dataset',
  },
  submit: {
    suggest_model: 'Sugerir um modelo',
    model_name: 'Nome do modelo',
    precision: 'Precisão',
    email: 'E-mail',
    model_placeholder: 'org/model',
    precision_placeholder: 'ex.: bf16, fp8',
    email_placeholder: 'voce@exemplo.com',
    submit: 'Enviar',
    submitting: 'Enviando…',
    thanks_touch: 'Obrigado! Entraremos em contato.',
    thanks_review: 'Obrigado! Iremos revisar.',
    suggest_task: 'Sugerir tarefa/dataset',
    task_key: 'Chave da tarefa',
    task_name: 'Nome da tarefa',
    group: 'Categoria / Grupo',
    select: 'Selecionar…',
    spanish_group: 'Espanhol (latam_es)',
    portuguese_group: 'Português (latam_pr)',
    other_group: 'Outro (latam_ts)',
    dataset_url: 'URL do dataset',
    url_placeholder: 'https://huggingface.co/datasets/...',
    short_desc: 'Descrição breve',
    short_desc_placeholder: 'O que esta tarefa avalia?',
    contact_email_label: 'E-mail de contato',
    submit_task: 'Enviar tarefa',
    not_sure: 'Sem saber por onde começar?',
    get_in_touch: 'Fale conosco',
  },
  about: {
    title: 'Sobre o LATAM Leaderboard',
    p1:
      'O LATAM Leaderboard é uma iniciativa comunitária dedicada a estabelecer padrões de avaliação orientados a tarefas e transparentes para sistemas de IA na América Latina. Medimos trabalho real, não apenas fluência.',
    p2:
      'Mais que um ranking, é a base para tornar a região um polo global de inovação em IA. Pesquisadores, desenvolvedores e entusiastas são bem-vindos para construir esses padrões.',
    mission_title: 'Nossa missão',
    mission_p:
      'Criamos benchmarks rigorosos, regionais e orientados a tarefas, em espanhol e português e em setores da LATAM, para que modelos sejam treinados e escolhidos com impacto real.',
    why_title: 'Por que isso importa',
    culture_sub: 'Da cultura à capacidade',
    culture_p1:
      'A avaliação em IA é dominada por benchmarks em inglês que não capturam as necessidades latino-americanas. Estamos mudando isso com padrões rigorosos e regionais.',
    culture_p2:
      'Capturar especificidades regionais é essencial, mas não basta. O gargalo é a avaliação: sem sinais claros por tarefa, modelos não seguem instruções nem entregam resultados.',
    signals_sub: 'Sinais para aprendizado (e RL)',
    signals_p:
      'Com o avanço de métodos baseados em reforço e feedback, precisamos de critérios de sucesso auditáveis. Nossos benchmarks podem servir de sinais de treino/validação.',
    bridge_sub: 'Reduzindo a lacuna de avaliação',
    bridge_p:
      'Faltam avaliações de qualidade em espanhol e português, sobretudo para tarefas concretas. Isso dificulta decisões e o progresso.',
    how_title: 'Como avaliamos modelos',
    how_p: 'Executamos benchmarks públicos baseados em forks do lm-evaluation-harness, focando em:',
    how_li1: 'Definições de tarefa com critérios de aceitação.',
    how_li2: 'Configurações reprodutíveis (versionamento, seeds e dados).',
    how_li3: 'Métricas orientadas ao resultado.',
    how_li4: 'Relatórios transparentes (o que cobre e o que não).',
    evolving_title: 'Padrões em evolução',
    evolving_li1: 'Fase 1: compreensão de linguagem.',
    evolving_li2: 'Fase 2: tarefas do mundo real (tradução, transcrição, resumo, instruções e saídas estruturadas).',
    evolving_li3: 'Fase 3: benchmarks da comunidade e datasets especializados, com formatos úteis para treino e RL.',
    community_title: 'Colaboração da comunidade',
    aips_sub: 'Para PMs de IA',
    aips_p: 'Traduza resultados de negócio em benchmarks e sinais de treinamento.',
    research_sub: 'Para pesquisadores e universidades',
    research_p: 'Contribua com benchmarks, metodologias e datasets alinhados às necessidades reais.',
    devs_sub: 'Para desenvolvedores de modelos',
    devs_p: 'Mostre desempenho em tarefas regionais e obtenha insights acionáveis.',
    companies_sub: 'Para empresas e profissionais',
    companies_p: 'Acesse dados confiáveis para guiar implementações. Proponha suas tarefas reais.',
    commit_title: 'Nosso compromisso',
    commit_li1: 'Ciência aberta: benchmarks, código e resultados públicos.',
    commit_li2: 'Rigor acadêmico: metodologia transparente; sem encenação.',
    commit_li3: 'Foco regional: línguas e tarefas que geram produtividade.',
    commit_li4: 'Protagonismo comunitário: a comunidade define os padrões.',
    future_title: 'Ajude a construir o futuro da IA na LATAM',
    future_p: 'Mais que ranking, somos a camada de avaliação para soluções reais.',
    ready_title: 'Pronto para contribuir?',
    ready_p: 'Com apoio de infraestrutura da Surus e da comunidade LATAM de IA.',
  },
}

export const translations: Record<Locale, TranslationDict> = { en, es, pt }


