{
    "tasks": {
        "enem_challenge": {
            "name": "enem_challenge",
            "group": "latam_pr",
            "description": "Multiple Choice Questions from the ENEM Exam",
            "long_description": "University Entrance Exam as a Guiding Test for Artificial Intelligence\nhttps://www.ime.usp.br/~ddm/project/enem/ENEM-GuidingTest.pdf\n\nThe ENEM Challenge consists in designing an autonomous system that matches the performance of a human students on the exam. The overall goal is to foster and evaluate the development of Artificial Intelligence techniques that have good performance on complex cognitive tasks, not particularly designed for AI systems. In addition, this challenge aims to promote and give more visiblity to the development of NLP tools for Brazilian Portuguese.\n\nHomepage: https://www.ime.usp.br/~ddm/project/enem",
            "fewshot": 3,
            "URL": "https://huggingface.co/datasets/eduagarcia/enem_challenge"
        },
        "assin2_rte": {
            "name": "assin2_rte",
            "group": "latam_pr",
            "description": "Portuguese RTE (Recognizing Textual Entailment) from ASSIN2",
            "long_description": "ASSIN2 (Avaliação de Similaridade Semântica e de Inferência Textual) is a Brazilian Portuguese benchmark. This RTE configuration predicts whether a hypothesis is entailed by a premise for sentence pairs in PT-BR, enabling evaluation of textual inference capabilities.",
            "fewshot": 2,
            "URL": "https://huggingface.co/datasets/nilc-nlp/assin2"
        },
        "assin2_sts": {
            "name": "assin2_sts",
            "group": "latam_pr",
            "description": "Portuguese STS (Semantic Textual Similarity) from ASSIN2",
            "long_description": "The ASSIN2 STS track measures semantic similarity between pairs of Brazilian Portuguese sentences on a 1–5 scale. It evaluates fine-grained understanding of meaning and paraphrase in PT-BR.",
            "fewshot": 15,
            "URL": "https://huggingface.co/datasets/nilc-nlp/assin2"
        },
        "bluex": {
            "name": "bluex",
            "group": "latam_pr",
            "description": "Portuguese BLUEX benchmark",
            "long_description": "BLUEX is a multiple-choice benchmark built from Brazilian university entrance exams (vestibulares), covering diverse subjects in Portuguese. This configuration uses a version without images to allow pure text evaluation.",
            "fewshot": 3,
            "URL": "https://huggingface.co/datasets/eduagarcia-temp/BLUEX_without_images"
        },
        "faquad_nli": {
            "name": "faquad_nli",
            "group": "latam_pr",
            "description": "Portuguese FaQuAD NLI (Natural Language Inference)",
            "long_description": "FaQuAD NLI reframes question–answer pairs from the Portuguese FaQuAD dataset as a binary NLI task. The model must judge if the answer satisfactorily addresses the question (Sim/Não), testing pragmatic understanding in PT-BR.",
            "fewshot": 15,
            "URL": "https://huggingface.co/datasets/ruanchaves/faquad-nli"
        },
        "oab_exams": {
            "name": "oab_exams",
            "group": "latam_pr",
            "description": "Portuguese OAB (Brazilian Bar Association) exams",
            "long_description": "Multiple-choice questions from Brazil's OAB Bar examinations across legal domains. Evaluates legal reasoning, reading comprehension, and knowledge of Brazilian law in Portuguese.",
            "fewshot": 3,
            "URL": "https://huggingface.co/datasets/eduagarcia/oab_exams"
        },
        "copa_es": {
            "name": "copa_es",
            "group": "latam_es",
            "description": "Spanish COPA (Choice of Plausible Alternatives)",
            "long_description": "Spanish version of COPA, a commonsense causal reasoning benchmark. Given a premise and a relation (cause/effect), the model selects the more plausible alternative in Spanish.",
            "fewshot": 1,
            "URL": "https://huggingface.co/datasets/BSC-LT/COPA-es"
        },
        "escola": {
            "name": "escola",
            "group": "latam_es",
            "description": "Spanish EsCoLA (Spanish Corpus of Linguistic Acceptability)",
            "long_description": "EsCoLA contains Spanish sentences annotated for linguistic acceptability. The task is a binary judgment (acceptable vs. unacceptable), probing grammatical knowledge and fluency in Spanish.",
            "fewshot": 1,
            "URL": "https://huggingface.co/datasets/nbel/EsCoLA"
        },
        "mgsm_direct_es_spanish_bench": {
            "name": "mgsm_direct_es_spanish_bench",
            "group": "latam_es",
            "description": "Spanish MGSM (Multilingual Grade School Math)",
            "long_description": "Spanish subset of MGSM, a multilingual grade-school math benchmark. This configuration expects direct numeric answers (no chain-of-thought), evaluating arithmetic and reasoning in Spanish.",
            "fewshot": 1,
            "URL": "https://huggingface.co/datasets/juletxara/mgsm"
        },
        "openbookqa_es": {
            "name": "openbookqa_es",
            "group": "latam_es",
            "description": "Spanish OpenBookQA",
            "long_description": "Spanish adaptation of OpenBookQA, a multiple-choice science QA benchmark requiring use of elementary science facts plus commonsense reasoning, localized to Spanish.",
            "fewshot": 1,
            "URL": "https://huggingface.co/datasets/BSC-LT/openbookqa-es"
        },
        "paws_es_spanish_bench": {
            "name": "paws_es_spanish_bench",
            "group": "latam_es",
            "description": "Spanish PAWS (Paraphrase Adversaries from Word Scrambling)",
            "long_description": "PAWS-X Spanish evaluates paraphrase identification with high-lexical-overlap sentence pairs. The model must determine whether two Spanish sentences are paraphrases.",
            "fewshot": 1,
            "URL": "https://huggingface.co/datasets/google-research-datasets/paws-x"
        },
        "wnli_es": {
            "name": "wnli_es",
            "group": "latam_es",
            "description": "Spanish WNLI (Winograd Natural Language Inference)",
            "long_description": "Spanish WNLI is a localized Winograd-style NLI task. Given two sentences, the model decides whether the second follows (Verdadero) from the first or not (Falso).",
            "fewshot": 1,
            "URL": "https://huggingface.co/datasets/PlanTL-GOB-ES/wnli-es"
        },
        "xnli_es_spanish_bench": {
            "name": "xnli_es_spanish_bench",
            "group": "latam_es",
            "description": "Spanish XNLI (Cross-lingual Natural Language Inference)",
            "long_description": "Spanish portion of XNLI, a cross-lingual entailment benchmark with three labels (entailment, neutral, contradiction). Assesses sentence-level inference in Spanish.",
            "fewshot": 1,
            "URL": "https://huggingface.co/datasets/facebook/xnli"
        },
        "teleia_cervantes_ave": {
            "name": "teleia_cervantes_ave",
            "group": "latam_es",
            "description": "Teleia Cervantes AVE assessment",
            "long_description": "Teleia Spanish assessment suite – Cervantes AVE subtask. Multiple-choice questions targeting Spanish language competency (reading and grammar) with dataset_name=cervantes_ave.",
            "fewshot": 1,
            "URL": "https://huggingface.co/datasets/migonsa/teleia"
        },
        "teleia_pce": {
            "name": "teleia_pce",
            "group": "latam_es",
            "description": "Teleia PCE (Prueba de Conocimientos Específicos)",
            "long_description": "Teleia Spanish assessment suite – PCE (specific knowledge) subtask. Three-option multiple-choice questions that evaluate formal Spanish knowledge and usage.",
            "fewshot": 1,
            "URL": "https://huggingface.co/datasets/migonsa/teleia"
        },
        "teleia_siele": {
            "name": "teleia_siele",
            "group": "latam_es",
            "description": "Teleia SIELE (Servicio Internacional de Evaluación de la Lengua Española)",
            "long_description": "Teleia Spanish assessment suite – SIELE-inspired subtask. Three-option multiple-choice items covering comprehension and grammatical accuracy in Spanish.",
            "fewshot": 1,
            "URL": "https://huggingface.co/datasets/migonsa/teleia"
        }
    }
}