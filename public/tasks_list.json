{
    "tasks": {
        "enem_challenge": {
            "name": "enem_challenge",
            "group": "latam_pr",
          "description": "Multiple Choice Questions from the ENEM Exam",
          "description_en": "Multiple-choice questions from the ENEM exam",
          "description_es": "Preguntas de opción múltiple del examen ENEM",
          "description_pt": "Questões de múltipla escolha do exame ENEM",
            "long_description": "University Entrance Exam as a Guiding Test for Artificial Intelligence\nhttps://www.ime.usp.br/~ddm/project/enem/ENEM-GuidingTest.pdf\n\nThe ENEM Challenge consists in designing an autonomous system that matches the performance of a human students on the exam. The overall goal is to foster and evaluate the development of Artificial Intelligence techniques that have good performance on complex cognitive tasks, not particularly designed for AI systems. In addition, this challenge aims to promote and give more visiblity to the development of NLP tools for Brazilian Portuguese.\n\nHomepage: https://www.ime.usp.br/~ddm/project/enem",
          "long_description_en": "University Entrance Exam as a Guiding Test for Artificial Intelligence\nhttps://www.ime.usp.br/~ddm/project/enem/ENEM-GuidingTest.pdf\n\nThe ENEM Challenge consists of designing an autonomous system that matches the performance of human students on the exam. The goal is to foster and evaluate AI techniques that perform well on complex cognitive tasks not specifically designed for AI, while promoting the development of NLP tools for Brazilian Portuguese.\n\nHomepage: https://www.ime.usp.br/~ddm/project/enem",
          "long_description_es": "Examen de ingreso universitario como prueba guía para la inteligencia artificial\nhttps://www.ime.usp.br/~ddm/project/enem/ENEM-GuidingTest.pdf\n\nEl Desafío ENEM consiste en diseñar un sistema autónomo que iguale el desempeño de estudiantes humanos en el examen. El objetivo es fomentar y evaluar técnicas de IA que funcionen bien en tareas cognitivas complejas no diseñadas específicamente para IA, además de promover el desarrollo de herramientas de PLN para el portugués brasileño.\n\nPágina: https://www.ime.usp.br/~ddm/project/enem",
          "long_description_pt": "Exame de ingresso universitário como teste orientador para inteligência artificial\nhttps://www.ime.usp.br/~ddm/project/enem/ENEM-GuidingTest.pdf\n\nO Desafio ENEM consiste em projetar um sistema autônomo que atinja o desempenho de estudantes humanos no exame. O objetivo é fomentar e avaliar técnicas de IA com bom desempenho em tarefas cognitivas complexas não projetadas especificamente para IA, além de promover o desenvolvimento de ferramentas de PLN para o português do Brasil.\n\nPágina: https://www.ime.usp.br/~ddm/project/enem",
            "fewshot": 3,
            "URL": "https://huggingface.co/datasets/eduagarcia/enem_challenge"
        },
        "assin2_rte": {
            "name": "assin2_rte",
            "group": "latam_pr",
          "description": "Portuguese RTE (Recognizing Textual Entailment) from ASSIN2",
          "description_en": "Portuguese RTE (Recognizing Textual Entailment) from ASSIN2",
          "description_es": "RTE en portugués (reconocimiento de inferencia textual) de ASSIN2",
          "description_pt": "RTE em português (Reconhecimento de Entailment Textual) do ASSIN2",
            "long_description": "ASSIN2 (Avaliação de Similaridade Semântica e de Inferência Textual) is a Brazilian Portuguese benchmark. This RTE configuration predicts whether a hypothesis is entailed by a premise for sentence pairs in PT-BR, enabling evaluation of textual inference capabilities.",
          "long_description_en": "ASSIN2 (Semantic Similarity and Textual Inference Evaluation) is a Brazilian Portuguese benchmark. This RTE setup predicts whether a hypothesis is entailed by a premise for PT-BR sentence pairs, enabling evaluation of textual inference.",
          "long_description_es": "ASSIN2 (Evaluación de Similitud Semántica e Inferencia Textual) es un benchmark en portugués de Brasil. Esta configuración de RTE predice si una hipótesis se infiere de una premisa para pares de oraciones en PT-BR, evaluando la inferencia textual.",
          "long_description_pt": "ASSIN2 (Avaliação de Similaridade Semântica e de Inferência Textual) é um benchmark em português do Brasil. Esta configuração de RTE prevê se uma hipótese é implicada por uma premissa em pares de frases em PT-BR, avaliando a inferência textual.",
            "fewshot": 2,
            "URL": "https://huggingface.co/datasets/nilc-nlp/assin2"
        },
        "assin2_sts": {
            "name": "assin2_sts",
            "group": "latam_pr",
          "description": "Portuguese STS (Semantic Textual Similarity) from ASSIN2",
          "description_en": "Portuguese STS (Semantic Textual Similarity) from ASSIN2",
          "description_es": "STS en portugués (similitud semántica textual) de ASSIN2",
          "description_pt": "STS em português (Similaridade Textual Semântica) do ASSIN2",
            "long_description": "The ASSIN2 STS track measures semantic similarity between pairs of Brazilian Portuguese sentences on a 1–5 scale. It evaluates fine-grained understanding of meaning and paraphrase in PT-BR.",
          "long_description_en": "The ASSIN2 STS track measures semantic similarity between pairs of Brazilian Portuguese sentences on a 1–5 scale, evaluating nuanced understanding of meaning and paraphrase in PT-BR.",
          "long_description_es": "La pista STS de ASSIN2 mide la similitud semántica entre pares de oraciones en portugués de Brasil en una escala de 1–5, evaluando la comprensión fina del significado y la paráfrasis en PT-BR.",
          "long_description_pt": "A trilha STS do ASSIN2 mede a similaridade semântica entre pares de frases em português do Brasil em uma escala de 1–5, avaliando a compreensão detalhada de significado e paráfrase em PT-BR.",
            "fewshot": 15,
            "URL": "https://huggingface.co/datasets/nilc-nlp/assin2"
        },
        "bluex": {
            "name": "bluex",
            "group": "latam_pr",
          "description": "Portuguese BLUEX benchmark",
          "description_en": "Portuguese BLUEX benchmark",
          "description_es": "Benchmark portugués BLUEX",
          "description_pt": "Benchmark português BLUEX",
            "long_description": "BLUEX is a multiple-choice benchmark built from Brazilian university entrance exams (vestibulares), covering diverse subjects in Portuguese. This configuration uses a version without images to allow pure text evaluation.",
          "long_description_en": "BLUEX is a multiple-choice benchmark built from Brazilian university entrance exams (vestibulares), covering diverse subjects in Portuguese. This setup uses a version without images to allow text-only evaluation.",
          "long_description_es": "BLUEX es un benchmark de opción múltiple construido a partir de exámenes de ingreso universitario brasileños (vestibulares), que cubre diversas materias en portugués. Esta configuración sin imágenes permite evaluación solo con texto.",
          "long_description_pt": "BLUEX é um benchmark de múltipla escolha construído a partir de vestibulares brasileiros, cobrindo diversas disciplinas em português. Esta configuração sem imagens permite avaliação apenas em texto.",
            "fewshot": 3,
            "URL": "https://huggingface.co/datasets/eduagarcia-temp/BLUEX_without_images"
        },
        "faquad_nli": {
            "name": "faquad_nli",
            "group": "latam_pr",
          "description": "Portuguese FaQuAD NLI (Natural Language Inference)",
          "description_en": "Portuguese FaQuAD NLI (Natural Language Inference)",
          "description_es": "NLI en portugués FaQuAD (inferencia de lenguaje natural)",
          "description_pt": "NLI em português FaQuAD (inferência de linguagem natural)",
            "long_description": "FaQuAD NLI reframes question–answer pairs from the Portuguese FaQuAD dataset as a binary NLI task. The model must judge if the answer satisfactorily addresses the question (Sim/Não), testing pragmatic understanding in PT-BR.",
          "long_description_en": "FaQuAD NLI reframes Q&A pairs from the Portuguese FaQuAD dataset as a binary NLI task. The model must judge whether the answer adequately addresses the question (Sim/Não), testing pragmatic understanding in PT-BR.",
          "long_description_es": "FaQuAD NLI reformula pares de pregunta–respuesta del dataset FaQuAD en portugués como una tarea binaria de NLI. El modelo debe juzgar si la respuesta aborda adecuadamente la pregunta (Sim/Não), evaluando comprensión pragmática en PT-BR.",
          "long_description_pt": "FaQuAD NLI reformula pares de pergunta–resposta do conjunto FaQuAD em português como uma tarefa binária de NLI. O modelo deve julgar se a resposta atende adequadamente à pergunta (Sim/Não), testando a compreensão pragmática em PT-BR.",
            "fewshot": 15,
            "URL": "https://huggingface.co/datasets/ruanchaves/faquad-nli"
        },
        "oab_exams": {
            "name": "oab_exams",
            "group": "latam_pr",
          "description": "Portuguese OAB (Brazilian Bar Association) exams",
          "description_en": "Portuguese OAB (Brazilian Bar Association) exams",
          "description_es": "Exámenes de la OAB (abogacía brasileña) en portugués",
          "description_pt": "Exames da OAB (Ordem dos Advogados do Brasil) em português",
            "long_description": "Multiple-choice questions from Brazil's OAB Bar examinations across legal domains. Evaluates legal reasoning, reading comprehension, and knowledge of Brazilian law in Portuguese.",
          "long_description_en": "Multiple-choice questions from Brazil's OAB Bar exams across legal domains. Evaluates legal reasoning, reading comprehension, and knowledge of Brazilian law in Portuguese.",
          "long_description_es": "Preguntas de opción múltiple de los exámenes de la OAB de Brasil en distintos ámbitos jurídicos. Evalúa razonamiento legal, comprensión lectora y conocimiento del derecho brasileño en portugués.",
          "long_description_pt": "Questões de múltipla escolha dos exames da OAB do Brasil em várias áreas do direito. Avalia raciocínio jurídico, compreensão de leitura e conhecimento do direito brasileiro em português.",
            "fewshot": 3,
            "URL": "https://huggingface.co/datasets/eduagarcia/oab_exams"
        },
        "copa_es": {
            "name": "copa_es",
            "group": "latam_es",
          "description": "Spanish COPA (Choice of Plausible Alternatives)",
          "description_en": "Spanish COPA (Choice of Plausible Alternatives)",
          "description_es": "COPA en español (elección de alternativas plausibles)",
          "description_pt": "COPA em espanhol (escolha de alternativas plausíveis)",
            "long_description": "Spanish version of COPA, a commonsense causal reasoning benchmark. Given a premise and a relation (cause/effect), the model selects the more plausible alternative in Spanish.",
          "long_description_en": "Spanish version of COPA, a commonsense causal reasoning benchmark. Given a premise and a relation (cause/effect), the model selects the more plausible alternative in Spanish.",
          "long_description_es": "Versión en español de COPA, un benchmark de razonamiento causal de sentido común. Dada una premisa y una relación (causa/efecto), el modelo elige la alternativa más plausible en español.",
          "long_description_pt": "Versão em espanhol do COPA, benchmark de raciocínio causal de senso comum. Dada uma premissa e uma relação (causa/efeito), o modelo escolhe a alternativa mais plausível em espanhol.",
            "fewshot": 1,
            "URL": "https://huggingface.co/datasets/BSC-LT/COPA-es"
        },
        "escola": {
            "name": "escola",
            "group": "latam_es",
          "description": "Spanish EsCoLA (Spanish Corpus of Linguistic Acceptability)",
          "description_en": "Spanish EsCoLA (Spanish Corpus of Linguistic Acceptability)",
          "description_es": "EsCoLA en español (Corpus de Aceptabilidad Lingüística en Español)",
          "description_pt": "EsCoLA em espanhol (Corpus de Aceitabilidade Linguística em Espanhol)",
            "long_description": "EsCoLA contains Spanish sentences annotated for linguistic acceptability. The task is a binary judgment (acceptable vs. unacceptable), probing grammatical knowledge and fluency in Spanish.",
          "long_description_en": "EsCoLA contains Spanish sentences annotated for linguistic acceptability. The task is a binary judgment (acceptable vs. unacceptable), probing grammatical knowledge and fluency in Spanish.",
          "long_description_es": "EsCoLA contiene oraciones en español anotadas por aceptabilidad lingüística. Es un juicio binario (aceptable vs. inaceptable) que explora conocimiento gramatical y fluidez.",
          "long_description_pt": "EsCoLA contém frases em espanhol anotadas quanto à aceitabilidade linguística. A tarefa é um julgamento binário (aceitável vs. inaceitável) que explora conhecimento gramatical e fluência.",
            "fewshot": 1,
            "URL": "https://huggingface.co/datasets/nbel/EsCoLA"
        },
        "mgsm_direct_es_spanish_bench": {
            "name": "mgsm_direct_es_spanish_bench",
            "group": "latam_es",
          "description": "Spanish MGSM (Multilingual Grade School Math)",
          "description_en": "Spanish MGSM (Multilingual Grade School Math)",
          "description_es": "MGSM en español (matemática escolar multilingüe)",
          "description_pt": "MGSM em espanhol (matemática escolar multilíngue)",
            "long_description": "Spanish subset of MGSM, a multilingual grade-school math benchmark. This configuration expects direct numeric answers (no chain-of-thought), evaluating arithmetic and reasoning in Spanish.",
          "long_description_en": "Spanish subset of MGSM, a multilingual grade-school math benchmark. This configuration expects direct numeric answers (no chain-of-thought), evaluating arithmetic and reasoning in Spanish.",
          "long_description_es": "Subconjunto en español de MGSM, benchmark multilingüe de matemática escolar. Esta configuración espera respuestas numéricas directas (sin cadena de razonamiento), evaluando aritmética y razonamiento en español.",
          "long_description_pt": "Subconjunto em espanhol do MGSM, benchmark multilíngue de matemática escolar. Esta configuração espera respostas numéricas diretas (sem cadeia de raciocínio), avaliando aritmética e raciocínio em espanhol.",
            "fewshot": 1,
            "URL": "https://huggingface.co/datasets/juletxara/mgsm"
        },
        "openbookqa_es": {
            "name": "openbookqa_es",
            "group": "latam_es",
          "description": "Spanish OpenBookQA",
          "description_en": "Spanish OpenBookQA",
          "description_es": "OpenBookQA en español",
          "description_pt": "OpenBookQA em espanhol",
            "long_description": "Spanish adaptation of OpenBookQA, a multiple-choice science QA benchmark requiring use of elementary science facts plus commonsense reasoning, localized to Spanish.",
          "long_description_en": "Spanish adaptation of OpenBookQA, a multiple-choice science QA benchmark requiring use of elementary science facts plus commonsense reasoning, localized to Spanish.",
          "long_description_es": "Adaptación al español de OpenBookQA, benchmark de preguntas de ciencia de opción múltiple que requiere usar hechos científicos elementales y razonamiento de sentido común.",
          "long_description_pt": "Adaptação em espanhol do OpenBookQA, benchmark de perguntas de ciência de múltipla escolha que exige fatos científicos básicos e raciocínio de senso comum.",
            "fewshot": 1,
            "URL": "https://huggingface.co/datasets/BSC-LT/openbookqa-es"
        },
        "paws_es_spanish_bench": {
            "name": "paws_es_spanish_bench",
            "group": "latam_es",
          "description": "Spanish PAWS (Paraphrase Adversaries from Word Scrambling)",
          "description_en": "Spanish PAWS (Paraphrase Adversaries from Word Scrambling)",
          "description_es": "PAWS en español (paráfrasis con alta superposición léxica)",
          "description_pt": "PAWS em espanhol (paráfrases com alta sobreposição lexical)",
            "long_description": "PAWS-X Spanish evaluates paraphrase identification with high-lexical-overlap sentence pairs. The model must determine whether two Spanish sentences are paraphrases.",
          "long_description_en": "PAWS-X Spanish evaluates paraphrase identification with sentence pairs that have high lexical overlap. The model must decide whether two Spanish sentences are paraphrases.",
          "long_description_es": "PAWS-X en español evalúa la identificación de paráfrasis con pares de oraciones de alta superposición léxica. El modelo debe decidir si dos oraciones en español son paráfrasis.",
          "long_description_pt": "PAWS-X em espanhol avalia a identificação de paráfrases com pares de sentenças de alta sobreposição lexical. O modelo deve decidir se duas sentenças em espanhol são paráfrases.",
            "fewshot": 1,
            "URL": "https://huggingface.co/datasets/google-research-datasets/paws-x"
        },
        "wnli_es": {
            "name": "wnli_es",
            "group": "latam_es",
          "description": "Spanish WNLI (Winograd Natural Language Inference)",
          "description_en": "Spanish WNLI (Winograd Natural Language Inference)",
          "description_es": "WNLI en español (inferencia Winograd)",
          "description_pt": "WNLI em espanhol (inferência Winograd)",
            "long_description": "Spanish WNLI is a localized Winograd-style NLI task. Given two sentences, the model decides whether the second follows (Verdadero) from the first or not (Falso).",
          "long_description_en": "Spanish WNLI is a localized Winograd-style NLI task. Given two sentences, the model decides whether the second follows (Verdadero) from the first or not (Falso).",
          "long_description_es": "WNLI en español es una tarea de NLI de estilo Winograd localizada. Dadas dos oraciones, el modelo decide si la segunda se sigue (Verdadero) de la primera o no (Falso).",
          "long_description_pt": "WNLI em espanhol é uma tarefa de NLI no estilo Winograd. Dadas duas sentenças, o modelo decide se a segunda decorre (Verdadeiro) da primeira ou não (Falso).",
            "fewshot": 1,
            "URL": "https://huggingface.co/datasets/PlanTL-GOB-ES/wnli-es"
        },
        "xnli_es_spanish_bench": {
            "name": "xnli_es_spanish_bench",
            "group": "latam_es",
          "description": "Spanish XNLI (Cross-lingual Natural Language Inference)",
          "description_en": "Spanish XNLI (Cross-lingual Natural Language Inference)",
          "description_es": "XNLI en español (inferencia multilingüe)",
          "description_pt": "XNLI em espanhol (inferência multilíngue)",
            "long_description": "Spanish portion of XNLI, a cross-lingual entailment benchmark with three labels (entailment, neutral, contradiction). Assesses sentence-level inference in Spanish.",
          "long_description_en": "Spanish portion of XNLI, a cross-lingual entailment benchmark with three labels (entailment, neutral, contradiction). Assesses sentence-level inference in Spanish.",
          "long_description_es": "Parte en español de XNLI, benchmark de inferencia multilingüe con tres etiquetas (entailment, neutral, contradiction). Evalúa inferencia a nivel de oración en español.",
          "long_description_pt": "Porção em espanhol do XNLI, benchmark de inferência multilíngue com três rótulos (entailment, neutral, contradiction). Avalia a inferência em nível de sentença em espanhol.",
            "fewshot": 1,
            "URL": "https://huggingface.co/datasets/facebook/xnli"
        },
        "teleia_cervantes_ave": {
            "name": "teleia_cervantes_ave",
            "group": "latam_es",
          "description": "Teleia Cervantes AVE assessment",
          "description_en": "Teleia Cervantes AVE assessment",
          "description_es": "Evaluación Teleia Cervantes AVE",
          "description_pt": "Avaliação Teleia Cervantes AVE",
            "long_description": "Teleia Spanish assessment suite – Cervantes AVE subtask. Multiple-choice questions targeting Spanish language competency (reading and grammar) with dataset_name=cervantes_ave.",
          "long_description_en": "Teleia Spanish assessment suite – Cervantes AVE subtask. Multiple-choice questions targeting Spanish language competency (reading and grammar) with dataset_name=cervantes_ave.",
          "long_description_es": "Suite de evaluación de español Teleia – subtarea Cervantes AVE. Preguntas de opción múltiple enfocadas en competencia en español (comprensión y gramática), con dataset_name=cervantes_ave.",
          "long_description_pt": "Suíte de avaliação de espanhol Teleia – subtarefa Cervantes AVE. Questões de múltipla escolha focadas em competência em espanhol (leitura e gramática), com dataset_name=cervantes_ave.",
            "fewshot": 1,
            "URL": "https://huggingface.co/datasets/migonsa/teleia"
        },
        "teleia_pce": {
            "name": "teleia_pce",
            "group": "latam_es",
          "description": "Teleia PCE (Prueba de Conocimientos Específicos)",
          "description_en": "Teleia PCE (Specific Knowledge Test)",
          "description_es": "Teleia PCE (Prueba de Conocimientos Específicos)",
          "description_pt": "Teleia PCE (Prova de Conhecimentos Específicos)",
            "long_description": "Teleia Spanish assessment suite – PCE (specific knowledge) subtask. Three-option multiple-choice questions that evaluate formal Spanish knowledge and usage.",
          "long_description_en": "Teleia Spanish assessment suite – PCE (specific knowledge) subtask. Three-option multiple-choice questions that evaluate formal Spanish knowledge and usage.",
          "long_description_es": "Suite de evaluación de español Teleia – subtarea PCE (conocimientos específicos). Preguntas de opción múltiple de tres opciones que evalúan conocimiento y uso formal del español.",
          "long_description_pt": "Suíte de avaliação de espanhol Teleia – subtarefa PCE (conhecimentos específicos). Questões de múltipla escolha com três opções que avaliam conhecimento e uso formal do espanhol.",
            "fewshot": 1,
            "URL": "https://huggingface.co/datasets/migonsa/teleia"
        },
        "teleia_siele": {
            "name": "teleia_siele",
            "group": "latam_es",
          "description": "Teleia SIELE (Servicio Internacional de Evaluación de la Lengua Española)",
          "description_en": "Teleia SIELE (International Spanish Language Evaluation Service)",
          "description_es": "Teleia SIELE (Servicio Internacional de Evaluación de la Lengua Española)",
          "description_pt": "Teleia SIELE (Serviço Internacional de Avaliação da Língua Espanhola)",
            "long_description": "Teleia Spanish assessment suite – SIELE-inspired subtask. Three-option multiple-choice items covering comprehension and grammatical accuracy in Spanish.",
          "long_description_en": "Teleia Spanish assessment suite – SIELE-inspired subtask. Three-option multiple-choice items covering comprehension and grammatical accuracy in Spanish.",
          "long_description_es": "Suite de evaluación de español Teleia – subtarea inspirada en SIELE. Ítems de opción múltiple con tres opciones que cubren comprensión y corrección gramatical en español.",
          "long_description_pt": "Suíte de avaliação de espanhol Teleia – subtarefa inspirada no SIELE. Itens de múltipla escolha com três opções cobrindo compreensão e correção gramatical em espanhol.",
            "fewshot": 1,
            "URL": "https://huggingface.co/datasets/migonsa/teleia"
        }
    }
}