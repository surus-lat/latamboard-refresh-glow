{
  "model_name": "Llama-3.1-8B-Instruct",
  "provider": "meta-llama",
  "categories": {
    "portuguese": {
      "task_scores": {
        "assin2_rte": {
          "score": 0.9066458374450974,
          "stderr": 0.0041625858277918905,
          "metric": "f1_macro",
          "alias": "assin2_rte"
        },
        "bluex": {
          "score": 0.6050069541029207,
          "stderr": 0.010520413566075377,
          "metric": "acc",
          "alias": "bluex"
        },
        "enem_challenge": {
          "score": 0.7137858642407278,
          "stderr": 0.006899928055249363,
          "metric": "acc",
          "alias": "enem_challenge"
        },
        "faquad_nli": {
          "score": 0.6224638987362054,
          "stderr": 0.017873891782898884,
          "metric": "f1_macro",
          "alias": "faquad_nli"
        },
        "oab_exams": {
          "score": 0.5184510250569476,
          "stderr": 0.006161385859008043,
          "metric": "acc",
          "alias": "oab_exams"
        }
      },
      "category_scores": {
        "latam_pr": {
          "score": 0.858905825296899,
          "stderr": 0.0027491055266944103,
          "metric": "acc",
          "alias": "latam_pr"
        }
      },
      "top_level_scores": {
        "latam_pr": {
          "score": 0.858905825296899,
          "stderr": 0.0027491055266944103,
          "metric": "acc",
          "alias": "latam_pr"
        }
      },
      "overall_score": 0.858905825296899,
      "model_name": "Llama-3.1-8B-Instruct",
      "model_name_sanitized": "unknown",
      "evaluation_time": "unknown"
    },
    "spanish": {
      "task_scores": {
        "spanish": {
          "score": 0.5849284099472495,
          "stderr": 0.005897396581785484,
          "metric": "acc",
          "alias": "spanish"
        },
        "copa_es": {
          "score": 0.812,
          "stderr": 0.01749067888034626,
          "metric": "acc",
          "alias": "copa_es"
        },
        "escola": {
          "score": 0.6657169990503324,
          "stderr": 0.014544342720025511,
          "metric": "acc",
          "alias": "escola"
        },
        "mgsm_direct_es_spanish_bench": {
          "score": 0.152,
          "stderr": 0.02275202449176547,
          "metric": "exact_match",
          "alias": "mgsm_direct_es_spanish_bench"
        },
        "openbookqa_es": {
          "score": 0.364,
          "stderr": 0.021539170637317653,
          "metric": "acc",
          "alias": "openbookqa_es"
        },
        "paws_es_spanish_bench": {
          "score": 0.639,
          "stderr": 0.010742308811391509,
          "metric": "acc",
          "alias": "paws_es_spanish_bench"
        },
        "teleia": {
          "score": 0.6666666666666666,
          "stderr": 0.10968472191337503,
          "metric": "acc_norm",
          "alias": "teleia"
        },
        "teleia_cervantes_ave": {
          "score": 0.5,
          "stderr": 0.22360679774997896,
          "metric": "acc",
          "alias": "teleia_cervantes_ave"
        },
        "teleia_pce": {
          "score": 0.5714285714285714,
          "stderr": 0.20203050891044214,
          "metric": "acc",
          "alias": "teleia_pce"
        },
        "teleia_siele": {
          "score": 1.0,
          "stderr": 0.0,
          "metric": "acc",
          "alias": "teleia_siele"
        },
        "wnli_es": {
          "score": 0.676056338028169,
          "stderr": 0.055934166129236386,
          "metric": "acc",
          "alias": "wnli_es"
        },
        "xnli_es_spanish_bench": {
          "score": 0.5024096385542168,
          "stderr": 0.010021956483067996,
          "metric": "acc",
          "alias": "xnli_es_spanish_bench"
        }
      },
      "category_scores": {
        "latam_es": {
          "score": 0.6367346163401434,
          "stderr": 0.005897396581785484,
          "metric": "acc",
          "alias": "latam_es"
        },
        "spanish": {
          "score": 0.5849284099472495,
          "stderr": 0.005897396581785484,
          "metric": "acc",
          "alias": "spanish"
        },
        "teleia": {
          "score": 0.6666666666666666,
          "stderr": 0.10968472191337503,
          "metric": "acc_norm",
          "alias": "teleia"
        }
      },
      "top_level_scores": {
        "latam_es": {
          "score": 0.6367346163401434,
          "stderr": 0.005897396581785484,
          "metric": "acc",
          "alias": "latam_es"
        },
        "spanish": {
          "score": 0.5849284099472495,
          "stderr": 0.005897396581785484,
          "metric": "acc",
          "alias": "spanish"
        },
        "teleia": {
          "score": 0.6666666666666666,
          "stderr": 0.10968472191337503,
          "metric": "acc_norm",
          "alias": "teleia"
        }
      },
      "overall_score": 0.6294432309846866,
      "model_name": "Llama-3.1-8B-Instruct",
      "model_name_sanitized": "unknown",
      "evaluation_time": "unknown"
    }
  },
  "overall_latam_score": 0.7478202208185212,
  "tasks_mapping": {
    "task_groups": {
      "latam_pr": {
        "name": "Portuguese LATAM",
        "description": "Portuguese language tasks for Latin America",
        "subtasks": [
          "assin2_rte",
          "assin2_sts",
          "bluex",
          "enem_challenge",
          "faquad_nli",
          "oab_exams"
        ]
      },
      "latam_es": {
        "name": "Spanish LATAM",
        "description": "Spanish language tasks for Latin America",
        "subtasks": [
          "spanish"
        ]
      },
      "spanish": {
        "name": "Spanish",
        "description": "Spanish language tasks",
        "subtasks": [
          "copa_es",
          "escola",
          "mgsm_direct_es_spanish_bench",
          "openbookqa_es",
          "paws_es_spanish_bench",
          "teleia",
          "wnli_es",
          "xnli_es_spanish_bench"
        ]
      },
      "teleia": {
        "name": "Teleia",
        "description": "Teleia Spanish language assessment tasks",
        "subtasks": [
          "teleia_cervantes_ave",
          "teleia_pce",
          "teleia_siele"
        ]
      }
    },
    "task_descriptions": {
      "assin2_rte": "Portuguese RTE (Recognizing Textual Entailment) from ASSIN2",
      "assin2_sts": "Portuguese STS (Semantic Textual Similarity) from ASSIN2",
      "bluex": "Portuguese BLUEX benchmark",
      "enem_challenge": "Portuguese ENEM (National High School Exam) challenge",
      "faquad_nli": "Portuguese FaQuAD NLI (Natural Language Inference)",
      "oab_exams": "Portuguese OAB (Brazilian Bar Association) exams",
      "copa_es": "Spanish COPA (Choice of Plausible Alternatives)",
      "escola": "Spanish EsCoLA (Spanish Corpus of Linguistic Acceptability)",
      "mgsm_direct_es_spanish_bench": "Spanish MGSM (Multilingual Grade School Math)",
      "openbookqa_es": "Spanish OpenBookQA",
      "paws_es_spanish_bench": "Spanish PAWS (Paraphrase Adversaries from Word Scrambling)",
      "teleia_cervantes_ave": "Teleia Cervantes AVE assessment",
      "teleia_pce": "Teleia PCE (Prueba de Conocimientos Espec\u00edficos)",
      "teleia_siele": "Teleia SIELE (Servicio Internacional de Evaluaci\u00f3n de la Lengua Espa\u00f1ola)",
      "wnli_es": "Spanish WNLI (Winograd Natural Language Inference)",
      "xnli_es_spanish_bench": "Spanish XNLI (Cross-lingual Natural Language Inference)"
    },
    "hierarchy": {
      "latam_pr": {
        "level": 1,
        "type": "top_level",
        "language": "portuguese"
      },
      "latam_es": {
        "level": 1,
        "type": "top_level",
        "language": "spanish"
      },
      "spanish": {
        "level": 2,
        "type": "category",
        "language": "spanish",
        "parent": "latam_es"
      },
      "teleia": {
        "level": 3,
        "type": "subcategory",
        "language": "spanish",
        "parent": "spanish"
      }
    }
  }
}