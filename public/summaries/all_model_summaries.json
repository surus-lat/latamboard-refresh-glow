{
  "gemma-3n-E2B-it": {
    "model_name": "gemma-3n-E2B-it",
    "provider": "unknown",
    "categories": {
      "portuguese": {
        "task_scores": {
          "assin2_rte": {
            "score": 0.9252429898198342,
            "stderr": 0.003760520912911408,
            "metric": "f1_macro",
            "alias": "assin2_rte"
          },
          "bluex": {
            "score": 0.5563282336578581,
            "stderr": 0.010672135581811038,
            "metric": "acc",
            "alias": "bluex"
          },
          "enem_challenge": {
            "score": 0.6801959412176347,
            "stderr": 0.007146051527313592,
            "metric": "acc",
            "alias": "enem_challenge"
          },
          "faquad_nli": {
            "score": 0.7057324514323299,
            "stderr": 0.013864388021898287,
            "metric": "f1_macro",
            "alias": "faquad_nli"
          },
          "oab_exams": {
            "score": 0.46879271070615036,
            "stderr": 0.006150194047688753,
            "metric": "acc",
            "alias": "oab_exams"
          }
        },
        "category_scores": {
          "latam_pr": {
            "score": 0.8553640983314567,
            "stderr": 0.005968246237718761,
            "metric": "acc",
            "alias": "latam_pr"
          }
        },
        "top_level_scores": {
          "latam_pr": {
            "score": 0.8553640983314567,
            "stderr": 0.005968246237718761,
            "metric": "acc",
            "alias": "latam_pr"
          }
        },
        "overall_score": 0.8553640983314567,
        "model_name": "gemma-3n-E2B-it",
        "model_name_sanitized": "unknown",
        "evaluation_time": "unknown"
      },
      "spanish": {
        "task_scores": {
          "spanish": {
            "score": 0.4470233609645818,
            "stderr": 0.0059363548710711575,
            "metric": "acc",
            "alias": "spanish"
          },
          "copa_es": {
            "score": 0.566,
            "stderr": 0.02218721580302901,
            "metric": "acc",
            "alias": "copa_es"
          },
          "escola": {
            "score": 0.5679012345679012,
            "stderr": 0.015272848692082622,
            "metric": "acc",
            "alias": "escola"
          },
          "mgsm_direct_es_spanish_bench": {
            "score": 0.016,
            "stderr": 0.00795166118887433,
            "metric": "exact_match",
            "alias": "mgsm_direct_es_spanish_bench"
          },
          "openbookqa_es": {
            "score": 0.226,
            "stderr": 0.01872295644913999,
            "metric": "acc",
            "alias": "openbookqa_es"
          },
          "paws_es_spanish_bench": {
            "score": 0.527,
            "stderr": 0.011166819105029964,
            "metric": "acc",
            "alias": "paws_es_spanish_bench"
          },
          "teleia": {
            "score": 0.5238095238095238,
            "stderr": 0.09515538736731587,
            "metric": "acc_norm",
            "alias": "teleia"
          },
          "teleia_cervantes_ave": {
            "score": 0.0,
            "stderr": 0.0,
            "metric": "acc",
            "alias": "teleia_cervantes_ave"
          },
          "teleia_pce": {
            "score": 0.2857142857142857,
            "stderr": 0.18442777839082938,
            "metric": "acc",
            "alias": "teleia_pce"
          },
          "teleia_siele": {
            "score": 0.75,
            "stderr": 0.16366341767699427,
            "metric": "acc",
            "alias": "teleia_siele"
          },
          "wnli_es": {
            "score": 0.676056338028169,
            "stderr": 0.055934166129236386,
            "metric": "acc",
            "alias": "wnli_es"
          },
          "xnli_es_spanish_bench": {
            "score": 0.3461847389558233,
            "stderr": 0.009536061379898198,
            "metric": "acc",
            "alias": "xnli_es_spanish_bench"
          }
        },
        "category_scores": {
          "latam_es": {
            "score": 0.43831739969624217,
            "stderr": 0.0059363548710711575,
            "metric": "acc",
            "alias": "latam_es"
          },
          "spanish": {
            "score": 0.4470233609645818,
            "stderr": 0.0059363548710711575,
            "metric": "acc",
            "alias": "spanish"
          },
          "teleia": {
            "score": 0.5238095238095238,
            "stderr": 0.09515538736731587,
            "metric": "acc_norm",
            "alias": "teleia"
          }
        },
        "top_level_scores": {
          "latam_es": {
            "score": 0.43831739969624217,
            "stderr": 0.0059363548710711575,
            "metric": "acc",
            "alias": "latam_es"
          },
          "spanish": {
            "score": 0.4470233609645818,
            "stderr": 0.0059363548710711575,
            "metric": "acc",
            "alias": "spanish"
          },
          "teleia": {
            "score": 0.5238095238095238,
            "stderr": 0.09515538736731587,
            "metric": "acc_norm",
            "alias": "teleia"
          }
        },
        "overall_score": 0.46971676149011593,
        "model_name": "gemma-3n-E2B-it",
        "model_name_sanitized": "unknown",
        "evaluation_time": "unknown"
      }
    },
    "overall_latam_score": 0.6468407490138495,
    "tasks_mapping": {
      "task_groups": {
        "latam_pr": {
          "name": "Portuguese LATAM",
          "description": "Portuguese language tasks for Latin America",
          "subtasks": [
            "assin2_rte",
            "assin2_sts",
            "bluex",
            "enem_challenge",
            "faquad_nli",
            "oab_exams"
          ]
        },
        "latam_es": {
          "name": "Spanish LATAM",
          "description": "Spanish language tasks for Latin America",
          "subtasks": [
            "spanish"
          ]
        },
        "spanish": {
          "name": "Spanish",
          "description": "Spanish language tasks",
          "subtasks": [
            "copa_es",
            "escola",
            "mgsm_direct_es_spanish_bench",
            "openbookqa_es",
            "paws_es_spanish_bench",
            "teleia",
            "wnli_es",
            "xnli_es_spanish_bench"
          ]
        },
        "teleia": {
          "name": "Teleia",
          "description": "Teleia Spanish language assessment tasks",
          "subtasks": [
            "teleia_cervantes_ave",
            "teleia_pce",
            "teleia_siele"
          ]
        }
      },
      "task_descriptions": {
        "assin2_rte": "Portuguese RTE (Recognizing Textual Entailment) from ASSIN2",
        "assin2_sts": "Portuguese STS (Semantic Textual Similarity) from ASSIN2",
        "bluex": "Portuguese BLUEX benchmark",
        "enem_challenge": "Portuguese ENEM (National High School Exam) challenge",
        "faquad_nli": "Portuguese FaQuAD NLI (Natural Language Inference)",
        "oab_exams": "Portuguese OAB (Brazilian Bar Association) exams",
        "copa_es": "Spanish COPA (Choice of Plausible Alternatives)",
        "escola": "Spanish EsCoLA (Spanish Corpus of Linguistic Acceptability)",
        "mgsm_direct_es_spanish_bench": "Spanish MGSM (Multilingual Grade School Math)",
        "openbookqa_es": "Spanish OpenBookQA",
        "paws_es_spanish_bench": "Spanish PAWS (Paraphrase Adversaries from Word Scrambling)",
        "teleia_cervantes_ave": "Teleia Cervantes AVE assessment",
        "teleia_pce": "Teleia PCE (Prueba de Conocimientos Espec\u00edficos)",
        "teleia_siele": "Teleia SIELE (Servicio Internacional de Evaluaci\u00f3n de la Lengua Espa\u00f1ola)",
        "wnli_es": "Spanish WNLI (Winograd Natural Language Inference)",
        "xnli_es_spanish_bench": "Spanish XNLI (Cross-lingual Natural Language Inference)"
      },
      "hierarchy": {
        "latam_pr": {
          "level": 1,
          "type": "top_level",
          "language": "portuguese"
        },
        "latam_es": {
          "level": 1,
          "type": "top_level",
          "language": "spanish"
        },
        "spanish": {
          "level": 2,
          "type": "category",
          "language": "spanish",
          "parent": "latam_es"
        },
        "teleia": {
          "level": 3,
          "type": "subcategory",
          "language": "spanish",
          "parent": "spanish"
        }
      }
    }
  },
  "Llama-3.1-8B-Instruct": {
    "model_name": "Llama-3.1-8B-Instruct",
    "provider": "meta-llama",
    "categories": {
      "portuguese": {
        "task_scores": {
          "assin2_rte": {
            "score": 0.9066458374450974,
            "stderr": 0.0041625858277918905,
            "metric": "f1_macro",
            "alias": "assin2_rte"
          },
          "bluex": {
            "score": 0.6050069541029207,
            "stderr": 0.010520413566075377,
            "metric": "acc",
            "alias": "bluex"
          },
          "enem_challenge": {
            "score": 0.7137858642407278,
            "stderr": 0.006899928055249363,
            "metric": "acc",
            "alias": "enem_challenge"
          },
          "faquad_nli": {
            "score": 0.6224638987362054,
            "stderr": 0.017873891782898884,
            "metric": "f1_macro",
            "alias": "faquad_nli"
          },
          "oab_exams": {
            "score": 0.5184510250569476,
            "stderr": 0.006161385859008043,
            "metric": "acc",
            "alias": "oab_exams"
          }
        },
        "category_scores": {
          "latam_pr": {
            "score": 0.858905825296899,
            "stderr": 0.0027491055266944103,
            "metric": "acc",
            "alias": "latam_pr"
          }
        },
        "top_level_scores": {
          "latam_pr": {
            "score": 0.858905825296899,
            "stderr": 0.0027491055266944103,
            "metric": "acc",
            "alias": "latam_pr"
          }
        },
        "overall_score": 0.858905825296899,
        "model_name": "Llama-3.1-8B-Instruct",
        "model_name_sanitized": "unknown",
        "evaluation_time": "unknown"
      },
      "spanish": {
        "task_scores": {
          "spanish": {
            "score": 0.5849284099472495,
            "stderr": 0.005897396581785484,
            "metric": "acc",
            "alias": "spanish"
          },
          "copa_es": {
            "score": 0.812,
            "stderr": 0.01749067888034626,
            "metric": "acc",
            "alias": "copa_es"
          },
          "escola": {
            "score": 0.6657169990503324,
            "stderr": 0.014544342720025511,
            "metric": "acc",
            "alias": "escola"
          },
          "mgsm_direct_es_spanish_bench": {
            "score": 0.152,
            "stderr": 0.02275202449176547,
            "metric": "exact_match",
            "alias": "mgsm_direct_es_spanish_bench"
          },
          "openbookqa_es": {
            "score": 0.364,
            "stderr": 0.021539170637317653,
            "metric": "acc",
            "alias": "openbookqa_es"
          },
          "paws_es_spanish_bench": {
            "score": 0.639,
            "stderr": 0.010742308811391509,
            "metric": "acc",
            "alias": "paws_es_spanish_bench"
          },
          "teleia": {
            "score": 0.6666666666666666,
            "stderr": 0.10968472191337503,
            "metric": "acc_norm",
            "alias": "teleia"
          },
          "teleia_cervantes_ave": {
            "score": 0.5,
            "stderr": 0.22360679774997896,
            "metric": "acc",
            "alias": "teleia_cervantes_ave"
          },
          "teleia_pce": {
            "score": 0.5714285714285714,
            "stderr": 0.20203050891044214,
            "metric": "acc",
            "alias": "teleia_pce"
          },
          "teleia_siele": {
            "score": 1.0,
            "stderr": 0.0,
            "metric": "acc",
            "alias": "teleia_siele"
          },
          "wnli_es": {
            "score": 0.676056338028169,
            "stderr": 0.055934166129236386,
            "metric": "acc",
            "alias": "wnli_es"
          },
          "xnli_es_spanish_bench": {
            "score": 0.5024096385542168,
            "stderr": 0.010021956483067996,
            "metric": "acc",
            "alias": "xnli_es_spanish_bench"
          }
        },
        "category_scores": {
          "latam_es": {
            "score": 0.6367346163401434,
            "stderr": 0.005897396581785484,
            "metric": "acc",
            "alias": "latam_es"
          },
          "spanish": {
            "score": 0.5849284099472495,
            "stderr": 0.005897396581785484,
            "metric": "acc",
            "alias": "spanish"
          },
          "teleia": {
            "score": 0.6666666666666666,
            "stderr": 0.10968472191337503,
            "metric": "acc_norm",
            "alias": "teleia"
          }
        },
        "top_level_scores": {
          "latam_es": {
            "score": 0.6367346163401434,
            "stderr": 0.005897396581785484,
            "metric": "acc",
            "alias": "latam_es"
          },
          "spanish": {
            "score": 0.5849284099472495,
            "stderr": 0.005897396581785484,
            "metric": "acc",
            "alias": "spanish"
          },
          "teleia": {
            "score": 0.6666666666666666,
            "stderr": 0.10968472191337503,
            "metric": "acc_norm",
            "alias": "teleia"
          }
        },
        "overall_score": 0.6294432309846866,
        "model_name": "Llama-3.1-8B-Instruct",
        "model_name_sanitized": "unknown",
        "evaluation_time": "unknown"
      }
    },
    "overall_latam_score": 0.7478202208185212,
    "tasks_mapping": {
      "task_groups": {
        "latam_pr": {
          "name": "Portuguese LATAM",
          "description": "Portuguese language tasks for Latin America",
          "subtasks": [
            "assin2_rte",
            "assin2_sts",
            "bluex",
            "enem_challenge",
            "faquad_nli",
            "oab_exams"
          ]
        },
        "latam_es": {
          "name": "Spanish LATAM",
          "description": "Spanish language tasks for Latin America",
          "subtasks": [
            "spanish"
          ]
        },
        "spanish": {
          "name": "Spanish",
          "description": "Spanish language tasks",
          "subtasks": [
            "copa_es",
            "escola",
            "mgsm_direct_es_spanish_bench",
            "openbookqa_es",
            "paws_es_spanish_bench",
            "teleia",
            "wnli_es",
            "xnli_es_spanish_bench"
          ]
        },
        "teleia": {
          "name": "Teleia",
          "description": "Teleia Spanish language assessment tasks",
          "subtasks": [
            "teleia_cervantes_ave",
            "teleia_pce",
            "teleia_siele"
          ]
        }
      },
      "task_descriptions": {
        "assin2_rte": "Portuguese RTE (Recognizing Textual Entailment) from ASSIN2",
        "assin2_sts": "Portuguese STS (Semantic Textual Similarity) from ASSIN2",
        "bluex": "Portuguese BLUEX benchmark",
        "enem_challenge": "Portuguese ENEM (National High School Exam) challenge",
        "faquad_nli": "Portuguese FaQuAD NLI (Natural Language Inference)",
        "oab_exams": "Portuguese OAB (Brazilian Bar Association) exams",
        "copa_es": "Spanish COPA (Choice of Plausible Alternatives)",
        "escola": "Spanish EsCoLA (Spanish Corpus of Linguistic Acceptability)",
        "mgsm_direct_es_spanish_bench": "Spanish MGSM (Multilingual Grade School Math)",
        "openbookqa_es": "Spanish OpenBookQA",
        "paws_es_spanish_bench": "Spanish PAWS (Paraphrase Adversaries from Word Scrambling)",
        "teleia_cervantes_ave": "Teleia Cervantes AVE assessment",
        "teleia_pce": "Teleia PCE (Prueba de Conocimientos Espec\u00edficos)",
        "teleia_siele": "Teleia SIELE (Servicio Internacional de Evaluaci\u00f3n de la Lengua Espa\u00f1ola)",
        "wnli_es": "Spanish WNLI (Winograd Natural Language Inference)",
        "xnli_es_spanish_bench": "Spanish XNLI (Cross-lingual Natural Language Inference)"
      },
      "hierarchy": {
        "latam_pr": {
          "level": 1,
          "type": "top_level",
          "language": "portuguese"
        },
        "latam_es": {
          "level": 1,
          "type": "top_level",
          "language": "spanish"
        },
        "spanish": {
          "level": 2,
          "type": "category",
          "language": "spanish",
          "parent": "latam_es"
        },
        "teleia": {
          "level": 3,
          "type": "subcategory",
          "language": "spanish",
          "parent": "spanish"
        }
      }
    }
  },
  "Qwen3-4B-Instruct-2507": {
    "model_name": "Qwen3-4B-Instruct-2507",
    "provider": "qwen",
    "categories": {
      "portuguese": {
        "task_scores": {
          "assin2_rte": {
            "score": 0.9259409687905688,
            "stderr": 0.0037441720298816723,
            "metric": "f1_macro",
            "alias": "assin2_rte"
          },
          "bluex": {
            "score": 0.674547983310153,
            "stderr": 0.010095304444252447,
            "metric": "acc",
            "alias": "bluex"
          },
          "enem_challenge": {
            "score": 0.7550734779566131,
            "stderr": 0.006577506644838695,
            "metric": "acc",
            "alias": "enem_challenge"
          },
          "faquad_nli": {
            "score": 0.8111730511071222,
            "stderr": 0.012435902570526361,
            "metric": "f1_macro",
            "alias": "faquad_nli"
          },
          "oab_exams": {
            "score": 0.5143507972665148,
            "stderr": 0.006154911506965476,
            "metric": "acc",
            "alias": "oab_exams"
          }
        },
        "category_scores": {
          "latam_pr": {
            "score": 0.8838324586851316,
            "stderr": 0.0021671846624677004,
            "metric": "acc",
            "alias": "latam_pr"
          }
        },
        "top_level_scores": {
          "latam_pr": {
            "score": 0.8838324586851316,
            "stderr": 0.0021671846624677004,
            "metric": "acc",
            "alias": "latam_pr"
          }
        },
        "overall_score": 0.8838324586851316,
        "model_name": "Qwen3-4B-Instruct-2507",
        "model_name_sanitized": "unknown",
        "evaluation_time": "unknown"
      },
      "spanish": {
        "task_scores": {
          "spanish": {
            "score": 0.560361718161266,
            "stderr": 0.005922294461518648,
            "metric": "acc",
            "alias": "spanish"
          },
          "copa_es": {
            "score": 0.74,
            "stderr": 0.019635965529725526,
            "metric": "acc",
            "alias": "copa_es"
          },
          "escola": {
            "score": 0.691358024691358,
            "stderr": 0.014242004840893833,
            "metric": "acc",
            "alias": "escola"
          },
          "mgsm_direct_es_spanish_bench": {
            "score": 0.32,
            "stderr": 0.02956172495524105,
            "metric": "exact_match",
            "alias": "mgsm_direct_es_spanish_bench"
          },
          "openbookqa_es": {
            "score": 0.298,
            "stderr": 0.020475118092988954,
            "metric": "acc",
            "alias": "openbookqa_es"
          },
          "paws_es_spanish_bench": {
            "score": 0.606,
            "stderr": 0.010928939603659186,
            "metric": "acc",
            "alias": "paws_es_spanish_bench"
          },
          "teleia": {
            "score": 0.6666666666666666,
            "stderr": 0.10968472191337503,
            "metric": "acc_norm",
            "alias": "teleia"
          },
          "teleia_cervantes_ave": {
            "score": 0.5,
            "stderr": 0.22360679774997896,
            "metric": "acc",
            "alias": "teleia_cervantes_ave"
          },
          "teleia_pce": {
            "score": 0.42857142857142855,
            "stderr": 0.20203050891044214,
            "metric": "acc",
            "alias": "teleia_pce"
          },
          "teleia_siele": {
            "score": 0.875,
            "stderr": 0.125,
            "metric": "acc",
            "alias": "teleia_siele"
          },
          "wnli_es": {
            "score": 0.7464788732394366,
            "stderr": 0.05199562688295701,
            "metric": "acc",
            "alias": "wnli_es"
          },
          "xnli_es_spanish_bench": {
            "score": 0.4791164658634538,
            "stderr": 0.010013327358568802,
            "metric": "acc",
            "alias": "xnli_es_spanish_bench"
          }
        },
        "category_scores": {
          "latam_es": {
            "score": 0.5960583102628529,
            "stderr": 0.005922294461518648,
            "metric": "acc",
            "alias": "latam_es"
          },
          "spanish": {
            "score": 0.560361718161266,
            "stderr": 0.005922294461518648,
            "metric": "acc",
            "alias": "spanish"
          },
          "teleia": {
            "score": 0.6666666666666666,
            "stderr": 0.10968472191337503,
            "metric": "acc_norm",
            "alias": "teleia"
          }
        },
        "top_level_scores": {
          "latam_es": {
            "score": 0.5960583102628529,
            "stderr": 0.005922294461518648,
            "metric": "acc",
            "alias": "latam_es"
          },
          "spanish": {
            "score": 0.560361718161266,
            "stderr": 0.005922294461518648,
            "metric": "acc",
            "alias": "spanish"
          },
          "teleia": {
            "score": 0.6666666666666666,
            "stderr": 0.10968472191337503,
            "metric": "acc_norm",
            "alias": "teleia"
          }
        },
        "overall_score": 0.6076955650302619,
        "model_name": "Qwen3-4B-Instruct-2507",
        "model_name_sanitized": "unknown",
        "evaluation_time": "unknown"
      }
    },
    "overall_latam_score": 0.7399453844739923,
    "tasks_mapping": {
      "task_groups": {
        "latam_pr": {
          "name": "Portuguese LATAM",
          "description": "Portuguese language tasks for Latin America",
          "subtasks": [
            "assin2_rte",
            "assin2_sts",
            "bluex",
            "enem_challenge",
            "faquad_nli",
            "oab_exams"
          ]
        },
        "latam_es": {
          "name": "Spanish LATAM",
          "description": "Spanish language tasks for Latin America",
          "subtasks": [
            "spanish"
          ]
        },
        "spanish": {
          "name": "Spanish",
          "description": "Spanish language tasks",
          "subtasks": [
            "copa_es",
            "escola",
            "mgsm_direct_es_spanish_bench",
            "openbookqa_es",
            "paws_es_spanish_bench",
            "teleia",
            "wnli_es",
            "xnli_es_spanish_bench"
          ]
        },
        "teleia": {
          "name": "Teleia",
          "description": "Teleia Spanish language assessment tasks",
          "subtasks": [
            "teleia_cervantes_ave",
            "teleia_pce",
            "teleia_siele"
          ]
        }
      },
      "task_descriptions": {
        "assin2_rte": "Portuguese RTE (Recognizing Textual Entailment) from ASSIN2",
        "assin2_sts": "Portuguese STS (Semantic Textual Similarity) from ASSIN2",
        "bluex": "Portuguese BLUEX benchmark",
        "enem_challenge": "Portuguese ENEM (National High School Exam) challenge",
        "faquad_nli": "Portuguese FaQuAD NLI (Natural Language Inference)",
        "oab_exams": "Portuguese OAB (Brazilian Bar Association) exams",
        "copa_es": "Spanish COPA (Choice of Plausible Alternatives)",
        "escola": "Spanish EsCoLA (Spanish Corpus of Linguistic Acceptability)",
        "mgsm_direct_es_spanish_bench": "Spanish MGSM (Multilingual Grade School Math)",
        "openbookqa_es": "Spanish OpenBookQA",
        "paws_es_spanish_bench": "Spanish PAWS (Paraphrase Adversaries from Word Scrambling)",
        "teleia_cervantes_ave": "Teleia Cervantes AVE assessment",
        "teleia_pce": "Teleia PCE (Prueba de Conocimientos Espec\u00edficos)",
        "teleia_siele": "Teleia SIELE (Servicio Internacional de Evaluaci\u00f3n de la Lengua Espa\u00f1ola)",
        "wnli_es": "Spanish WNLI (Winograd Natural Language Inference)",
        "xnli_es_spanish_bench": "Spanish XNLI (Cross-lingual Natural Language Inference)"
      },
      "hierarchy": {
        "latam_pr": {
          "level": 1,
          "type": "top_level",
          "language": "portuguese"
        },
        "latam_es": {
          "level": 1,
          "type": "top_level",
          "language": "spanish"
        },
        "spanish": {
          "level": 2,
          "type": "category",
          "language": "spanish",
          "parent": "latam_es"
        },
        "teleia": {
          "level": 3,
          "type": "subcategory",
          "language": "spanish",
          "parent": "spanish"
        }
      }
    }
  },
  "Llama-3.2-3B-Instruct": {
    "model_name": "Llama-3.2-3B-Instruct",
    "provider": "meta-llama",
    "categories": {
      "portuguese": {
        "task_scores": {
          "assin2_rte": {
            "score": 0.8326183308273833,
            "stderr": 0.00539640503197095,
            "metric": "f1_macro",
            "alias": "assin2_rte"
          },
          "bluex": {
            "score": 0.4909596662030598,
            "stderr": 0.010736422693483481,
            "metric": "acc",
            "alias": "bluex"
          },
          "enem_challenge": {
            "score": 0.5913226032190343,
            "stderr": 0.007510302782428625,
            "metric": "acc",
            "alias": "enem_challenge"
          },
          "faquad_nli": {
            "score": 0.6762358315211449,
            "stderr": 0.0164719844842371,
            "metric": "f1_macro",
            "alias": "faquad_nli"
          },
          "oab_exams": {
            "score": 0.4337129840546697,
            "stderr": 0.0061228719140914625,
            "metric": "acc",
            "alias": "oab_exams"
          }
        },
        "category_scores": {
          "latam_pr": {
            "score": 0.7962520642259387,
            "stderr": 0.004278421339604872,
            "metric": "acc",
            "alias": "latam_pr"
          }
        },
        "top_level_scores": {
          "latam_pr": {
            "score": 0.7962520642259387,
            "stderr": 0.004278421339604872,
            "metric": "acc",
            "alias": "latam_pr"
          }
        },
        "overall_score": 0.7962520642259387,
        "model_name": "Llama-3.2-3B-Instruct",
        "model_name_sanitized": "unknown",
        "evaluation_time": "unknown"
      },
      "spanish": {
        "task_scores": {
          "spanish": {
            "score": 0.505802562170309,
            "stderr": 0.006003425821330598,
            "metric": "acc",
            "alias": "spanish"
          },
          "copa_es": {
            "score": 0.734,
            "stderr": 0.019780559675655396,
            "metric": "acc",
            "alias": "copa_es"
          },
          "escola": {
            "score": 0.43779677113010446,
            "stderr": 0.01529590138441499,
            "metric": "acc",
            "alias": "escola"
          },
          "mgsm_direct_es_spanish_bench": {
            "score": 0.108,
            "stderr": 0.01966955938156875,
            "metric": "exact_match",
            "alias": "mgsm_direct_es_spanish_bench"
          },
          "openbookqa_es": {
            "score": 0.28,
            "stderr": 0.020099950647503192,
            "metric": "acc",
            "alias": "openbookqa_es"
          },
          "paws_es_spanish_bench": {
            "score": 0.583,
            "stderr": 0.011027978425535526,
            "metric": "acc",
            "alias": "paws_es_spanish_bench"
          },
          "teleia": {
            "score": 0.5238095238095238,
            "stderr": 0.10968472191337503,
            "metric": "acc",
            "alias": "teleia"
          },
          "teleia_cervantes_ave": {
            "score": 0.3333333333333333,
            "stderr": 0.210818510677892,
            "metric": "acc",
            "alias": "teleia_cervantes_ave"
          },
          "teleia_pce": {
            "score": 0.42857142857142855,
            "stderr": 0.20203050891044214,
            "metric": "acc",
            "alias": "teleia_pce"
          },
          "teleia_siele": {
            "score": 0.75,
            "stderr": 0.16366341767699427,
            "metric": "acc",
            "alias": "teleia_siele"
          },
          "wnli_es": {
            "score": 0.647887323943662,
            "stderr": 0.05708756925195616,
            "metric": "acc",
            "alias": "wnli_es"
          },
          "xnli_es_spanish_bench": {
            "score": 0.4678714859437751,
            "stderr": 0.010001361068172962,
            "metric": "acc",
            "alias": "xnli_es_spanish_bench"
          }
        },
        "category_scores": {
          "latam_es": {
            "score": 0.5180511492135893,
            "stderr": 0.006003425821330598,
            "metric": "acc",
            "alias": "latam_es"
          },
          "spanish": {
            "score": 0.505802562170309,
            "stderr": 0.006003425821330598,
            "metric": "acc",
            "alias": "spanish"
          },
          "teleia": {
            "score": 0.5238095238095238,
            "stderr": 0.10968472191337503,
            "metric": "acc",
            "alias": "teleia"
          }
        },
        "top_level_scores": {
          "latam_es": {
            "score": 0.5180511492135893,
            "stderr": 0.006003425821330598,
            "metric": "acc",
            "alias": "latam_es"
          },
          "spanish": {
            "score": 0.505802562170309,
            "stderr": 0.006003425821330598,
            "metric": "acc",
            "alias": "spanish"
          },
          "teleia": {
            "score": 0.5238095238095238,
            "stderr": 0.10968472191337503,
            "metric": "acc",
            "alias": "teleia"
          }
        },
        "overall_score": 0.515887745064474,
        "model_name": "Llama-3.2-3B-Instruct",
        "model_name_sanitized": "unknown",
        "evaluation_time": "unknown"
      }
    },
    "overall_latam_score": 0.657151606719764,
    "tasks_mapping": {
      "task_groups": {
        "latam_pr": {
          "name": "Portuguese LATAM",
          "description": "Portuguese language tasks for Latin America",
          "subtasks": [
            "assin2_rte",
            "assin2_sts",
            "bluex",
            "enem_challenge",
            "faquad_nli",
            "oab_exams"
          ]
        },
        "latam_es": {
          "name": "Spanish LATAM",
          "description": "Spanish language tasks for Latin America",
          "subtasks": [
            "spanish"
          ]
        },
        "spanish": {
          "name": "Spanish",
          "description": "Spanish language tasks",
          "subtasks": [
            "copa_es",
            "escola",
            "mgsm_direct_es_spanish_bench",
            "openbookqa_es",
            "paws_es_spanish_bench",
            "teleia",
            "wnli_es",
            "xnli_es_spanish_bench"
          ]
        },
        "teleia": {
          "name": "Teleia",
          "description": "Teleia Spanish language assessment tasks",
          "subtasks": [
            "teleia_cervantes_ave",
            "teleia_pce",
            "teleia_siele"
          ]
        }
      },
      "task_descriptions": {
        "assin2_rte": "Portuguese RTE (Recognizing Textual Entailment) from ASSIN2",
        "assin2_sts": "Portuguese STS (Semantic Textual Similarity) from ASSIN2",
        "bluex": "Portuguese BLUEX benchmark",
        "enem_challenge": "Portuguese ENEM (National High School Exam) challenge",
        "faquad_nli": "Portuguese FaQuAD NLI (Natural Language Inference)",
        "oab_exams": "Portuguese OAB (Brazilian Bar Association) exams",
        "copa_es": "Spanish COPA (Choice of Plausible Alternatives)",
        "escola": "Spanish EsCoLA (Spanish Corpus of Linguistic Acceptability)",
        "mgsm_direct_es_spanish_bench": "Spanish MGSM (Multilingual Grade School Math)",
        "openbookqa_es": "Spanish OpenBookQA",
        "paws_es_spanish_bench": "Spanish PAWS (Paraphrase Adversaries from Word Scrambling)",
        "teleia_cervantes_ave": "Teleia Cervantes AVE assessment",
        "teleia_pce": "Teleia PCE (Prueba de Conocimientos Espec\u00edficos)",
        "teleia_siele": "Teleia SIELE (Servicio Internacional de Evaluaci\u00f3n de la Lengua Espa\u00f1ola)",
        "wnli_es": "Spanish WNLI (Winograd Natural Language Inference)",
        "xnli_es_spanish_bench": "Spanish XNLI (Cross-lingual Natural Language Inference)"
      },
      "hierarchy": {
        "latam_pr": {
          "level": 1,
          "type": "top_level",
          "language": "portuguese"
        },
        "latam_es": {
          "level": 1,
          "type": "top_level",
          "language": "spanish"
        },
        "spanish": {
          "level": 2,
          "type": "category",
          "language": "spanish",
          "parent": "latam_es"
        },
        "teleia": {
          "level": 3,
          "type": "subcategory",
          "language": "spanish",
          "parent": "spanish"
        }
      }
    }
  },
  "gemma-3n-E4B-it": {
    "model_name": "gemma-3n-E4B-it",
    "provider": "unknown",
    "categories": {
      "portuguese": {
        "task_scores": {
          "assin2_rte": {
            "score": 0.9248184855940154,
            "stderr": 0.0037698749072707077,
            "metric": "f1_macro",
            "alias": "assin2_rte"
          },
          "bluex": {
            "score": 0.6022253129346314,
            "stderr": 0.010521744882814025,
            "metric": "acc",
            "alias": "bluex"
          },
          "enem_challenge": {
            "score": 0.7144856543037089,
            "stderr": 0.006911619014081894,
            "metric": "acc",
            "alias": "enem_challenge"
          },
          "faquad_nli": {
            "score": 0.713356171384681,
            "stderr": 0.013871457174472092,
            "metric": "f1_macro",
            "alias": "faquad_nli"
          },
          "oab_exams": {
            "score": 0.5234624145785877,
            "stderr": 0.006164760581515523,
            "metric": "acc",
            "alias": "oab_exams"
          }
        },
        "category_scores": {
          "latam_pr": {
            "score": 0.8620589987251935,
            "stderr": 0.0047272413356402015,
            "metric": "acc",
            "alias": "latam_pr"
          }
        },
        "top_level_scores": {
          "latam_pr": {
            "score": 0.8620589987251935,
            "stderr": 0.0047272413356402015,
            "metric": "acc",
            "alias": "latam_pr"
          }
        },
        "overall_score": 0.8620589987251935,
        "model_name": "gemma-3n-E4B-it",
        "model_name_sanitized": "unknown",
        "evaluation_time": "unknown"
      },
      "spanish": {
        "task_scores": {
          "spanish": {
            "score": 0.4461190655614167,
            "stderr": 0.005940292194493601,
            "metric": "acc",
            "alias": "spanish"
          },
          "copa_es": {
            "score": 0.556,
            "stderr": 0.022242244375731048,
            "metric": "acc",
            "alias": "copa_es"
          },
          "escola": {
            "score": 0.5679012345679012,
            "stderr": 0.015272848692082622,
            "metric": "acc",
            "alias": "escola"
          },
          "mgsm_direct_es_spanish_bench": {
            "score": 0.052,
            "stderr": 0.014070391025641647,
            "metric": "exact_match",
            "alias": "mgsm_direct_es_spanish_bench"
          },
          "openbookqa_es": {
            "score": 0.24,
            "stderr": 0.0191188666537598,
            "metric": "acc",
            "alias": "openbookqa_es"
          },
          "paws_es_spanish_bench": {
            "score": 0.528,
            "stderr": 0.01116558709462175,
            "metric": "acc",
            "alias": "paws_es_spanish_bench"
          },
          "teleia": {
            "score": 0.47619047619047616,
            "stderr": 0.10618323936714011,
            "metric": "acc_norm",
            "alias": "teleia"
          },
          "teleia_cervantes_ave": {
            "score": 0.0,
            "stderr": 0.0,
            "metric": "acc",
            "alias": "teleia_cervantes_ave"
          },
          "teleia_pce": {
            "score": 0.2857142857142857,
            "stderr": 0.18442777839082938,
            "metric": "acc",
            "alias": "teleia_pce"
          },
          "teleia_siele": {
            "score": 0.875,
            "stderr": 0.125,
            "metric": "acc",
            "alias": "teleia_siele"
          },
          "wnli_es": {
            "score": 0.6338028169014085,
            "stderr": 0.05758184314388002,
            "metric": "acc",
            "alias": "wnli_es"
          },
          "xnli_es_spanish_bench": {
            "score": 0.3429718875502008,
            "stderr": 0.009514999934033513,
            "metric": "acc",
            "alias": "xnli_es_spanish_bench"
          }
        },
        "category_scores": {
          "latam_es": {
            "score": 0.4477100249704218,
            "stderr": 0.005940292194493601,
            "metric": "acc",
            "alias": "latam_es"
          },
          "spanish": {
            "score": 0.4461190655614167,
            "stderr": 0.005940292194493601,
            "metric": "acc",
            "alias": "spanish"
          },
          "teleia": {
            "score": 0.47619047619047616,
            "stderr": 0.10618323936714011,
            "metric": "acc_norm",
            "alias": "teleia"
          }
        },
        "top_level_scores": {
          "latam_es": {
            "score": 0.4477100249704218,
            "stderr": 0.005940292194493601,
            "metric": "acc",
            "alias": "latam_es"
          },
          "spanish": {
            "score": 0.4461190655614167,
            "stderr": 0.005940292194493601,
            "metric": "acc",
            "alias": "spanish"
          },
          "teleia": {
            "score": 0.47619047619047616,
            "stderr": 0.10618323936714011,
            "metric": "acc_norm",
            "alias": "teleia"
          }
        },
        "overall_score": 0.45667318890743824,
        "model_name": "gemma-3n-E4B-it",
        "model_name_sanitized": "unknown",
        "evaluation_time": "unknown"
      }
    },
    "overall_latam_score": 0.6548845118478077,
    "tasks_mapping": {
      "task_groups": {
        "latam_pr": {
          "name": "Portuguese LATAM",
          "description": "Portuguese language tasks for Latin America",
          "subtasks": [
            "assin2_rte",
            "assin2_sts",
            "bluex",
            "enem_challenge",
            "faquad_nli",
            "oab_exams"
          ]
        },
        "latam_es": {
          "name": "Spanish LATAM",
          "description": "Spanish language tasks for Latin America",
          "subtasks": [
            "spanish"
          ]
        },
        "spanish": {
          "name": "Spanish",
          "description": "Spanish language tasks",
          "subtasks": [
            "copa_es",
            "escola",
            "mgsm_direct_es_spanish_bench",
            "openbookqa_es",
            "paws_es_spanish_bench",
            "teleia",
            "wnli_es",
            "xnli_es_spanish_bench"
          ]
        },
        "teleia": {
          "name": "Teleia",
          "description": "Teleia Spanish language assessment tasks",
          "subtasks": [
            "teleia_cervantes_ave",
            "teleia_pce",
            "teleia_siele"
          ]
        }
      },
      "task_descriptions": {
        "assin2_rte": "Portuguese RTE (Recognizing Textual Entailment) from ASSIN2",
        "assin2_sts": "Portuguese STS (Semantic Textual Similarity) from ASSIN2",
        "bluex": "Portuguese BLUEX benchmark",
        "enem_challenge": "Portuguese ENEM (National High School Exam) challenge",
        "faquad_nli": "Portuguese FaQuAD NLI (Natural Language Inference)",
        "oab_exams": "Portuguese OAB (Brazilian Bar Association) exams",
        "copa_es": "Spanish COPA (Choice of Plausible Alternatives)",
        "escola": "Spanish EsCoLA (Spanish Corpus of Linguistic Acceptability)",
        "mgsm_direct_es_spanish_bench": "Spanish MGSM (Multilingual Grade School Math)",
        "openbookqa_es": "Spanish OpenBookQA",
        "paws_es_spanish_bench": "Spanish PAWS (Paraphrase Adversaries from Word Scrambling)",
        "teleia_cervantes_ave": "Teleia Cervantes AVE assessment",
        "teleia_pce": "Teleia PCE (Prueba de Conocimientos Espec\u00edficos)",
        "teleia_siele": "Teleia SIELE (Servicio Internacional de Evaluaci\u00f3n de la Lengua Espa\u00f1ola)",
        "wnli_es": "Spanish WNLI (Winograd Natural Language Inference)",
        "xnli_es_spanish_bench": "Spanish XNLI (Cross-lingual Natural Language Inference)"
      },
      "hierarchy": {
        "latam_pr": {
          "level": 1,
          "type": "top_level",
          "language": "portuguese"
        },
        "latam_es": {
          "level": 1,
          "type": "top_level",
          "language": "spanish"
        },
        "spanish": {
          "level": 2,
          "type": "category",
          "language": "spanish",
          "parent": "latam_es"
        },
        "teleia": {
          "level": 3,
          "type": "subcategory",
          "language": "spanish",
          "parent": "spanish"
        }
      }
    }
  }
}