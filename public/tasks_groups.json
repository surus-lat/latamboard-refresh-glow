{
    "task_groups": {
    "latam_es": {
      "name": "Spanish LATAM",
      "description": "Spanish language tasks for Latin America",
      "long_description": "Suite of selected tasks from the Spanish Bench available in the lm-evaluation-harness from the team at [IberoBench](https://aclanthology.org/2025.coling-main.699/) and [SomosNLP's Spanish Leaderboard](https://github.com/somosnlp/lm-evaluation-harness) designed to evaluate the performance of models in the Spanish language. The tasks cover a wide range of linguistic abilities, from basic comprehension to complex reasoning in Spanish. The evaluation suite includes tasks like COPA for choice of plausible alternatives, ESCOLA for Spanish Corpus of Linguistic Acceptability, MGSM for Multilingual Grade School Math, OpenBookQA for open-domain question answering, PAWS for paraphrase adversaries from word scrambling, TELEIA for Teleia Spanish language assessment, WNLI for Winograd Natural Language Inference, and XNLI for Cross-lingual Natural Language Inference. This comprehensive set of benchmarks helps assess how well language models can process and generate Spanish text across different contexts and difficulty levels.",
      "repository": "https://github.com/EleutherAI/lm-evaluation-harness",
      "subtasks": [
        "copa_es",
        "escola",
        "mgsm_direct_es_spanish_bench",
        "openbookqa_es",
        "paws_es_spanish_bench",
        "teleia",
        "wnli_es",
        "xnli_es_spanish_bench"
      ]
    },
    "latam_pr": {
        "name": "Portuguese LATAM",
        "description": "Portuguese language tasks for Latin America",
        "long_description": "Suite of selected tasks from the [Portuguese leaderboard group](https://huggingface.co/spaces/eduagarcia/open_pt_llm_leaderboard) designed to evaluate the performance of models in the Portuguese language. Based on the work of the Open Portuguese LLM Leaderboard, these tasks were carefully selected to measure the capabilities of language models in understanding Portuguese. The tasks cover a wide range of linguistic abilities, from basic comprehension to complex reasoning in Portuguese. The evaluation suite includes tasks like ASSIN2 for textual entailment and semantic similarity, BLUEX for university entrance exams, and ENEM for standardized testing comprehension. This comprehensive set of benchmarks helps assess how well language models can process and generate Portuguese text across different contexts and difficulty levels.",
        "repository": "https://github.com/eduagarcia/lm-evaluation-harness-pt",
        "subtasks": [
          "assin2_rte",
          "assin2_sts", 
          "bluex",
          "enem_challenge",
          "faquad_nli",
          "oab_exams"
        ]
      }
    }
}